{
  
    
  
    
  
    
  
    
  
    
        "post4": {
            "title": "Pandas cheat sheet for data science",
            "content": "Pandas cheat sheet for data science . Statistics . Multi-variate analysis . Understand the problem. . Normal distribution python #histogram sns.distplot(df_train[‘SalePrice’]); . | Skewness/Kurtosis? #skewness and kurtosis print(&quot;Skewness: %f&quot; % df_train[&#39;SalePrice&#39;].skew()) print(&quot;Kurtosis: %f&quot; % df_train[&#39;SalePrice&#39;].kurt()) . | Show peakedness | . Univariable study . Relationship with numerical variables . #scatter plot grlivarea/saleprice var = &#39;GrLivArea&#39; data = pd.concat([df_train[&#39;SalePrice&#39;], df_train[var]], axis=1) data.plot.scatter(x=var, y=&#39;SalePrice&#39;, ylim=(0,800000)); . Relationship with categorical features . #box plot overallqual/saleprice var = &#39;OverallQual&#39; data = pd.concat([df_train[&#39;SalePrice&#39;], df_train[var]], axis=1) f, ax = plt.subplots(figsize=(8, 6)) fig = sns.boxplot(x=var, y=&quot;SalePrice&quot;, data=data) fig.axis(ymin=0, ymax=800000); . Multivariate study. We’ll try to understand how the dependent variable and independent variables relate. | Correlation matrix (heatmap style) . #correlation matrix corrmat = df_train.corr() f, ax = plt.subplots(figsize=(12, 9)) sns.heatmap(corrmat, vmax=.8, square=True); # and zoomed corr matrix k = 10 #number of variables for heatmap cols = corrmat.nlargest(k, &#39;SalePrice&#39;)[&#39;SalePrice&#39;].index cm = np.corrcoef(df_train[cols].values.T) sns.set(font_scale=1.25) hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt=&#39;.2f&#39;, annot_kws={&#39;size&#39;: 10}, yticklabels=cols.values, xticklabels=cols.values) plt.show() . . Scatter plots between target and correlated variables . #scatterplot sns.set() cols = [&#39;SalePrice&#39;, &#39;OverallQual&#39;, &#39;GrLivArea&#39;, &#39;GarageCars&#39;, &#39;TotalBsmtSF&#39;, &#39;FullBath&#39;, &#39;YearBuilt&#39;] sns.pairplot(df_train[cols], size = 2.5) plt.show(); . Basic cleaning. We’ll clean the dataset and handle the missing data, outliers and categorical variables. . #missing data total = df_train.isnull().sum().sort_values(ascending=False) percent = (df_train.isnull().sum()/df_train.isnull().count()).sort_values(ascending=False) missing_data = pd.concat([total, percent], axis=1, keys=[&#39;Total&#39;, &#39;Percent&#39;]) missing_data.head(20) . Test assumptions. We’ll check if our data meets the assumptions required by most multivariate techniques. . Feature understanding . Are the features continuous, discrete or none of the above? . | What is the distribution of this feature? . | Does the distribution largely depend on what subset of examples is being considered? Time-based segmentation? | Type-based segmentation? | . | Does this feature contain holes (missing values)? Are those holes possible to be filled, or would they stay forever? | If it possible to eliminate them in the future data? | . | Are there duplicate and/or intersecting examples? Answering this question right is extremely important, since duplicate or connected data points might significantly affect the results of model validation if not properly excluded. | . | Where do the features come from? Should we come up with the new features that prove to be useful, how hard would it be to incorporate those features in the final design? | . | Is the data real-time? Are the requests real-time? | . | If yes, well-engineered simple features would likely rock. If no, we likely are in the business of advanced models and algorithms. | . | Are there features that can be used as the “truth”? Plots . Supervised vs. unsupervised learning | classification vs. regression | Prediction vs. Inference 9. Baseline Modeling 10. Secondary Modeling 11. Communicating Results 12. Conclusion 13. Resources | | . Preliminaries . # import libraries (standard) import numpy as np import matplotlib.pyplot as plt import pandas as pd from pandas import DataFrame, Series . Import . https://chrisyeh96.github.io/2017/08/08/definitive-guide-python-imports.html . Input Output . Input . Empty DataFrame (top) . # dataframe empty df = DataFrame() . CSV (top) . # pandas read csv df = pd.read_csv(&#39;file.csv&#39;) df = pd.read_csv(&#39;file.csv&#39;, header=0, index_col=0, quotechar=&#39;&quot;&#39;,sep=&#39;:&#39;, na_values = [&#39;na&#39;, &#39;-&#39;, &#39;.&#39;, &#39;&#39;]) # specifying &quot;.&quot; and &quot;NA&quot; as missing values in the Last Name column and &quot;.&quot; as missing values in Pre-Test Score column df = pd.read_csv(&#39;../data/example.csv&#39;, na_values={&#39;Last Name&#39;: [&#39;.&#39;, &#39;NA&#39;], &#39;Pre-Test Score&#39;: [&#39;.&#39;]}) # skipping the top 3 rows df = pd.read_csv(&#39;../data/example.csv&#39;, na_values=sentinels, skiprows=3) # interpreting &quot;,&quot; in strings around numbers as thousands separators df = pd.read_csv(&#39;../data/example.csv&#39;, thousands=&#39;,&#39;) # `encoding=&#39;latin1&#39;`, `encoding=&#39;iso-8859-1&#39;` or `encoding=&#39;cp1252&#39;` df = pd.read_csv(&#39;example.csv&#39;,encoding=&#39;latin1&#39;) . CSV (Inline) (top) . # pandas read string from io import StringIO data = &quot;&quot;&quot;, Animal, Cuteness, Desirable row-1, dog, 8.7, True row-2, cat, 9.5, True row-3, bat, 2.6, False&quot;&quot;&quot; df = pd.read_csv(StringIO(data), header=0, index_col=0, skipinitialspace=True) . JSON (top) . # pandas read json import json json_data = open(&#39;data-text.json&#39;).read() data = json.loads(json_data) for item in data: print item . XML (top) . # pandas read xml from xml.etree import ElementTree as ET tree = ET.parse(&#39;../../data/chp3/data-text.xml&#39;) root = tree.getroot() print root data = root.find(&#39;Data&#39;) all_data = [] for observation in data: record = {} for item in observation: lookup_key = item.attrib.keys()[0] if lookup_key == &#39;Numeric&#39;: rec_key = &#39;NUMERIC&#39; rec_value = item.attrib[&#39;Numeric&#39;] else: rec_key = item.attrib[lookup_key] rec_value = item.attrib[&#39;Code&#39;] record[rec_key] = rec_value all_data.append(record) print all_data . Excel (top) . # pandas read excel # Each Excel sheet in a Python dictionary workbook = pd.ExcelFile(&#39;file.xlsx&#39;) d = {} # start with an empty dictionary for sheet_name in workbook.sheet_names: df = workbook.parse(sheet_name) d[sheet_name] = df . MySQL (top) . # pandas read sql import pymysql from sqlalchemy import create_engine engine = create_engine(&#39;mysql+pymysql://&#39; +&#39;USER:PASSWORD@HOST/DATABASE&#39;) df = pd.read_sql_table(&#39;table&#39;, engine) . Combine DataFrame (top) . # pandas concat dataframes # Example 1 ... s1 = Series(range(6)) s2 = s1 * s1 s2.index = s2.index + 2# misalign indexes df = pd.concat([s1, s2], axis=1) # Example 2 ... s3 = Series({&#39;Tom&#39;:1, &#39;Dick&#39;:4, &#39;Har&#39;:9}) s4 = Series({&#39;Tom&#39;:3, &#39;Dick&#39;:2, &#39;Mar&#39;:5}) df = pd.concat({&#39;A&#39;:s3, &#39;B&#39;:s4 }, axis=1) . From Dictionary (top) default — assume data is in columns . # pandas read dictionary df = DataFrame({ &#39;col0&#39; : [1.0, 2.0, 3.0, 4.0], &#39;col1&#39; : [100, 200, 300, 400] }) . use helper method for data in rows . # pandas read dictionary df = DataFrame.from_dict({ # data by row # rows as python dictionaries &#39;row0&#39; : {&#39;col0&#39;:0, &#39;col1&#39;:&#39;A&#39;}, &#39;row1&#39; : {&#39;col0&#39;:1, &#39;col1&#39;:&#39;B&#39;} }, orient=&#39;index&#39;) df = DataFrame.from_dict({ # data by row # rows as python lists &#39;row0&#39; : [1, 1+1j, &#39;A&#39;], &#39;row1&#39; : [2, 2+2j, &#39;B&#39;] }, orient=&#39;index&#39;) . from iterations of lists . # pandas read lists aa = [&#39;aa1&#39;, &#39;aa2&#39;, &#39;aa3&#39;, &#39;aa4&#39;, &#39;aa5&#39;] bb = [&#39;bb1&#39;, &#39;bb2&#39;, &#39;bb3&#39;, &#39;bb4&#39;, &#39;bb5&#39;] cc = [&#39;cc1&#39;, &#39;cc2&#39;, &#39;cc3&#39;, &#39;cc4&#39;, &#39;cc5&#39;] lists = [aa, bb, cc] pd.DataFrame(list(itertools.product(*lists)), columns=[&#39;aa&#39;, &#39;bb&#39;, &#39;cc&#39;]) . source: https://stackoverflow.com/questions/45672342/create-a-dataframe-of-permutations-in-pandas-from-list . Examples (top) — simple - default integer indexes . # pandas read random df = DataFrame(np.random.rand(50,5)) . — with a time-stamp row index: . # pandas read random timestamp df = DataFrame(np.random.rand(500,5)) df.index = pd.date_range(&#39;1/1/2005&#39;, periods=len(df), freq=&#39;M&#39;) . — with alphabetic row and col indexes and a “groupable” variable . import string import random r = 52 # note: min r is 1; max r is 52 c = 5 df = DataFrame(np.random.randn(r, c), columns = [&#39;col&#39;+str(i) for i in range(c)], index = list((string. ascii_uppercase+ string.ascii_lowercase)[0:r])) df[&#39;group&#39;] = list(&#39;&#39;.join(random.choice(&#39;abcde&#39;) for _ in range(r)) ) . Generate dataframe with 1 variable column . # pandas dataframe create final_biom_df = final_biom_df.append([pd.DataFrame({&#39;trial&#39; : curr_trial, &#39;biomarker_name&#39; : curr_biomarker, &#39;observation_id&#39; : curr_observation, &#39;visit&#39; : curr_timepoint, &#39;value&#39; : np.random.randint(low=1, high=100, size=30), &#39;unit&#39; : curr_unit, &#39;base&#39; : is_base, })]) . Reading files . # pandas read multiple files files = glob.glob(&#39;weather/*.csv&#39;) weather_dfs = [pd.read_csv(fp, names=columns) for fp in files] weather = pd.concat(weather_dfs) . Output . CSV (top) . df.to_csv(&#39;name.csv&#39;, encoding=&#39;utf-8&#39;) df.to_csv(&#39;filename.csv&#39;, header=False) . Excel . from pandas import ExcelWriter writer = ExcelWriter(&#39;filename.xlsx&#39;) df1.to_excel(writer,&#39;Sheet1&#39;) df2.to_excel(writer,&#39;Sheet2&#39;) writer.save() . MySQL (top) . import pymysql from sqlalchemy import create_engine e = create_engine(&#39;mysql+pymysql://&#39; + &#39;USER:PASSWORD@HOST/DATABASE&#39;) df.to_sql(&#39;TABLE&#39;,e, if_exists=&#39;replace&#39;) . Python object (top) . d = df.to_dict() # to dictionary str = df.to_string() # to string m = df.as_matrix() # to numpy matrix . JSON . ### orient=’records’ df.to_json(r&#39;Path to store the JSON file File Name.json&#39;,orient=&#39;records&#39;) [{&quot;Product&quot;:&quot;Desktop Computer&quot;,&quot;Price&quot;:700},{&quot;Product&quot;:&quot;Tablet&quot;,&quot;Price&quot;:250},{&quot;Produc . source: https://datatofish.com/export-pandas-dataframe-json/ . Exploration . pandas profiling . conda install -c anaconda pandas-profiling import pandas as pd import pandas_profiling # Depreciated: pre 2.0.0 version df = pd.read_csv(&#39;titanic/train.csv&#39;) #Pandas-Profiling 2.0.0 df.profile_report() # save as html profile = df.profile_report(title=&#39;Pandas Profiling Report&#39;) profile.to_file(output_file=&quot;output.html&quot;) . example report: link . source: link . overview missing data: . # dataframe missing data ted.isna().sum() . Selecting . Summary . Select columns . # dataframe select columns s = df[&#39;col_label&#39;] # returns Series df = df[[&#39;col_label&#39;]] # return DataFrame df = df[[&#39;L1&#39;, &#39;L2&#39;]] # select with list df = df[index] # select with index df = df[s] #select with Series . Select rows . # dataframe select rows df = df[&#39;from&#39;:&#39;inc_to&#39;]# label slice df = df[3:7] # integer slice df = df[df[&#39;col&#39;] &gt; 0.5]# Boolean Series df = df.loc[&#39;label&#39;] # single label df = df.loc[container] # lab list/Series df = df.loc[&#39;from&#39;:&#39;to&#39;]# inclusive slice df = df.loc[bs] # Boolean Series df = df.iloc[0] # single integer df = df.iloc[container] # int list/Series df = df.iloc[0:5] # exclusive slice df = df.ix[x] # loc then iloc . Select a cross-section (top) . # dataframe select slices # r and c can be scalar, list, slice df.loc[r, c] # label accessor (row, col) df.iloc[r, c]# integer accessor df.ix[r, c] # label access int fallback df[c].iloc[r]# chained – also for .loc . Select a cell (top) . # dataframe select cell # r and c must be label or integer df.at[r, c] # fast scalar label accessor df.iat[r, c] # fast scalar int accessor df[c].iat[r] # chained – also for .at . DataFrame indexing methods (top) . v = df.get_value(r, c) # get by row, col df = df.set_value(r,c,v)# set by row, col df = df.xs(key, axis) # get cross-section df = df.filter(items, like, regex, axis) df = df.select(crit, axis) . Some index attributes and methods (top) . # dataframe index atrributes # some Index attributes b = idx.is_monotonic_decreasing b = idx.is_monotonic_increasing b = idx.has_duplicates i = idx.nlevels # num of index levels # some Index methods idx = idx.astype(dtype)# change data type b = idx.equals(o) # check for equality idx = idx.union(o) # union of two indexes i = idx.nunique() # number unique labels label = idx.min() # minimum label label = idx.max() # maximum label . Whole DataFrame . Content/Structure . # dataframe get info df.info() # index &amp; data types dfh = df.head(i) # get first i rows dft = df.tail(i) # get last i rows dfs = df.describe() # summary stats cols top_left_corner_df = df.iloc[:4, :4] . Non-indexing attributes (top) . # dataframe non-indexing methods dfT = df.T # transpose rows and cols l = df.axes # list row and col indexes (r, c) = df.axes # from above s = df.dtypes # Series column data types b = df.empty # True for empty DataFrame i = df.ndim # number of axes (it is 2) t = df.shape # (row-count, column-count) i = df.size # row-count * column-count a = df.values # get a numpy array for df . Utilities - DataFrame utility methods (top) . # dataframe sort df = df.copy() # dataframe copy df = df.rank() # rank each col (default) df = df.sort([&#39;sales&#39;], ascending=[False]) df = df.sort_values(by=col) df = df.sort_values(by=[col1, col2]) df = df.sort_index() df = df.astype(dtype) # type conversion . Iterations (top) . # dataframe iterate for df.iteritems()# (col-index, Series) pairs df.iterrows() # (row-index, Series) pairs # example ... iterating over columns for (name, series) in df.iteritems(): print(&#39;Col name: &#39; + str(name)) print(&#39;First value: &#39; + str(series.iat[0]) + &#39; n&#39;) . Maths (top) . # dataframe math df = df.abs() # absolute values df = df.add(o) # add df, Series or value s = df.count() # non NA/null values df = df.cummax() # (cols default axis) df = df.cummin() # (cols default axis) df = df.cumsum() # (cols default axis) df = df.diff() # 1st diff (col def axis) df = df.div(o) # div by df, Series, value df = df.dot(o) # matrix dot product s = df.max() # max of axis (col def) s = df.mean() # mean (col default axis) s = df.median()# median (col default) s = df.min() # min of axis (col def) df = df.mul(o) # mul by df Series val s = df.sum() # sum axis (cols default) df = df.where(df &gt; 0.5, other=np.nan) . Select/filter (top) . # dataframe select filter df = df.filter(items=[&#39;a&#39;, &#39;b&#39;]) # by col df = df.filter(items=[5], axis=0) #by row df = df.filter(like=&#39;x&#39;) # keep x in col df = df.filter(regex=&#39;x&#39;) # regex in col df = df.select(lambda x: not x%5)#5th rows . Columns . Index and labels (top) . # dataframe get index idx = df.columns # get col index label = df.columns[0] # first col label l = df.columns.tolist() # list col labels . Data type conversions (top) . # dataframe convert column st = df[&#39;col&#39;].astype(str)# Series dtype a = df[&#39;col&#39;].values # numpy array pl = df[&#39;col&#39;].tolist() # python list . Note: useful dtypes for Series conversion: int, float, str . Common column-wide methods/attributes (top) . value = df[&#39;col&#39;].dtype # type of column value = df[&#39;col&#39;].size # col dimensions value = df[&#39;col&#39;].count()# non-NA count value = df[&#39;col&#39;].sum() value = df[&#39;col&#39;].prod() value = df[&#39;col&#39;].min() # column min value = df[&#39;col&#39;].max() # column max value = df[&#39;col&#39;].mean() # also median() value = df[&#39;col&#39;].cov(df[&#39;col2&#39;]) s = df[&#39;col&#39;].describe() s = df[&#39;col&#39;].value_counts() . Find index label for min/max values in column (top) . label = df[&#39;col1&#39;].idxmin() label = df[&#39;col1&#39;].idxmax() . Common column element-wise methods (top) . s = df[&#39;col&#39;].isnull() s = df[&#39;col&#39;].notnull() # not isnull() s = df[&#39;col&#39;].astype(float) s = df[&#39;col&#39;].abs() s = df[&#39;col&#39;].round(decimals=0) s = df[&#39;col&#39;].diff(periods=1) s = df[&#39;col&#39;].shift(periods=1) s = df[&#39;col&#39;].to_datetime() s = df[&#39;col&#39;].fillna(0) # replace NaN w 0 s = df[&#39;col&#39;].cumsum() s = df[&#39;col&#39;].cumprod() s = df[&#39;col&#39;].pct_change(periods=4) s = df[&#39;col&#39;].rolling_sum(periods=4, window=4) . Note: also rolling_min(), rolling_max(), and many more. . Position of a column index label (top) . j = df.columns.get_loc(&#39;col_name&#39;) . Column index values unique/monotonic (top) . if df.columns.is_unique: pass # ... b = df.columns.is_monotonic_increasing b = df.columns.is_monotonic_decreasing . Selecting . Columns (top) . s = df[&#39;colName&#39;] # select col to Series df = df[[&#39;colName&#39;]] # select col to df df = df[[&#39;a&#39;,&#39;b&#39;]] # select 2 or more df = df[[&#39;c&#39;,&#39;a&#39;,&#39;b&#39;]]# change col order s = df[df.columns[0]] # select by number df = df[df.columns[[0, 3, 4]] # by number s = df.pop(&#39;c&#39;) # get col &amp; drop from df . Columns with Python attributes (top) . s = df.a # same as s = df[&#39;a&#39;] # cannot create new columns by attribute df.existing_column = df.a / df.b df[&#39;new_column&#39;] = df.a / df.b . Selecting columns with .loc, .iloc and .ix (top) . df = df.loc[:, &#39;col1&#39;:&#39;col2&#39;] # inclusive df = df.iloc[:, 0:2] # exclusive . Conditional selection (top) . df.query(&#39;A &gt; C&#39;) df.query(&#39;A &gt; 0&#39;) df.query(&#39;A &gt; 0 &amp; A &lt; 1&#39;) df.query(&#39;A &gt; B | A &gt; C&#39;) df[df[&#39;coverage&#39;] &gt; 50] # all rows where coverage is more than 50 df[(df[&#39;deaths&#39;] &gt; 500) | (df[&#39;deaths&#39;] &lt; 50)] df[(df[&#39;score&#39;] &gt; 1) &amp; (df[&#39;score&#39;] &lt; 5)] df[~(df[&#39;regiment&#39;] == &#39;Dragoons&#39;)] # Select all the regiments not named &quot;Dragoons&quot; df[df[&#39;age&#39;].notnull() &amp; df[&#39;sex&#39;].notnull()] # ignore the missing data points . (top) . # is in df[df.name.isin(value_list)] # value_list = [&#39;Tina&#39;, &#39;Molly&#39;, &#39;Jason&#39;] df[~df.name.isin(value_list)] . Partial matching (top) . # column contains df2[df2.E.str.contains(&quot;tw|ou&quot;)] # column contains regex df[&#39;raw&#39;].str.contains(&#39;....-..-..&#39;, regex=True) # regex # dataframe column list contains selection = [&#39;cat&#39;, &#39;dog&#39;] df[pd.DataFrame(df.species.tolist()).isin(selection).any(1)] Out[64]: molecule species 0 a [dog] 2 c [cat, dog] 3 d [cat, horse, pig] . # dataframe column rename df.rename(columns={&#39;old1&#39;:&#39;new1&#39;,&#39;old2&#39;:&#39;new2&#39;}, inplace=True) df.columns = [&#39;a&#39;, &#39;b&#39;] . Manipulating . Adding (top) . df[&#39;new_col&#39;] = range(len(df)) df[&#39;new_col&#39;] = np.repeat(np.nan,len(df)) df[&#39;random&#39;] = np.random.rand(len(df)) df[&#39;index_as_col&#39;] = df.index df1[[&#39;b&#39;,&#39;c&#39;]] = df2[[&#39;e&#39;,&#39;f&#39;]] df3 = df1.append(other=df2) . Vectorised arithmetic on columns (top) . df[&#39;proportion&#39;]=df[&#39;count&#39;]/df[&#39;total&#39;] df[&#39;percent&#39;] = df[&#39;proportion&#39;] * 100.0 . Append a column of row sums to a DataFrame (top) . df[&#39;Total&#39;] = df.sum(axis=1) . Apply numpy mathematical functions to columns (top) . df[&#39;log_data&#39;] = np.log(df[&#39;col1&#39;]) . Set column values set based on criteria (top) . df[&#39;b&#39;]=df[&#39;a&#39;].where(df[&#39;a&#39;]&gt;0,other=0) df[&#39;d&#39;]=df[&#39;a&#39;].where(df.b!=0,other=df.c) . Swapping (top) . df[[&#39;B&#39;, &#39;A&#39;]] = df[[&#39;A&#39;, &#39;B&#39;]] . Dropping (top) . df = df.drop(&#39;col1&#39;, axis=1) df.drop(&#39;col1&#39;, axis=1, inplace=True) df = df.drop([&#39;col1&#39;,&#39;col2&#39;], axis=1) s = df.pop(&#39;col&#39;) # drops from frame del df[&#39;col&#39;] # even classic python works df.drop(df.columns[0], inplace=True) # drop columns with column names where the first three letters of the column names was &#39;pre&#39; cols = [c for c in df.columns if c.lower()[:3] != &#39;pre&#39;] df=df[cols] . Multiply every column in DataFrame by Series (top) . df = df.mul(s, axis=0) # on matched rows . Rows . Get Position (top) . a = np.where(df[&#39;col&#39;] &gt;= 2) #numpy array . DataFrames have same row index (top) . len(a)==len(b) and all(a.index==b.index) # Get the integer position of a row or col index label i = df.index.get_loc(&#39;row_label&#39;) . Row index values are unique/monotonic (top) . if df.index.is_unique: pass # ... b = df.index.is_monotonic_increasing b = df.index.is_monotonic_decreasing . Get the row index and labels (top) . idx = df.index # get row index label = df.index[0] # 1st row label lst = df.index.tolist() # get as a list . Change the (row) index (top) . df.index = idx # new ad hoc index df = df.set_index(&#39;A&#39;) # col A new index df = df.set_index([&#39;A&#39;, &#39;B&#39;]) # MultiIndex df = df.reset_index() # replace old w new . df.index = range(len(df)) # set with list df = df.reindex(index=range(len(df))) df = df.set_index(keys=[&#39;r1&#39;,&#39;r2&#39;,&#39;etc&#39;]) df.rename(index={&#39;old&#39;:&#39;new&#39;},inplace=True) . Selecting . By column values (top) . df = df[df[&#39;col2&#39;] &gt;= 0.0] df = df[(df[&#39;col3&#39;]&gt;=1.0) | (df[&#39;col1&#39;]&lt;0.0)] df = df[df[&#39;col&#39;].isin([1,2,5,7,11])] df = df[~df[&#39;col&#39;].isin([1,2,5,7,11])] df = df[df[&#39;col&#39;].str.contains(&#39;hello&#39;)] . Using isin over multiple columns (top) . ## fake up some data data = {1:[1,2,3], 2:[1,4,9], 3:[1,8,27]} df = DataFrame(data) # multi-column isin lf = {1:[1, 3], 3:[8, 27]} # look for f = df[df[list(lf)].isin(lf).all(axis=1)] Selecting rows using an index idx = df[df[&#39;col&#39;] &gt;= 2].index print(df.ix[idx]) . Slice of rows by integer position (top) . [inclusive-from : exclusive-to [: step]] default start is 0; default end is len(df) df = df[:] # copy DataFrame df = df[0:2] # rows 0 and 1 df = df[-1:] # the last row df = df[2:3] # row 2 (the third row) df = df[:-1] # all but the last row df = df[::2] # every 2nd row (0 2 ..) . Slice of rows by label/index (top) . [inclusive-from : inclusive–to [ : step]] df = df[&#39;a&#39;:&#39;c&#39;] # rows &#39;a&#39; through &#39;c&#39; . Manipulating . Adding rows . df = original_df.append(more_rows_in_df) . Append a row of column totals to a DataFrame (top) . # Option 1: use dictionary comprehension sums = {col: df[col].sum() for col in df} sums_df = DataFrame(sums,index=[&#39;Total&#39;]) df = df.append(sums_df) # Option 2: All done with pandas df = df.append(DataFrame(df.sum(), columns=[&#39;Total&#39;]).T) . Dropping rows (by name) (top) . df = df.drop(&#39;row_label&#39;) df = df.drop([&#39;row1&#39;,&#39;row2&#39;]) # multi-row . Drop duplicates in the row index (top) . df[&#39;index&#39;] = df.index # 1 create new col df = df.drop_duplicates(cols=&#39;index&#39;,take_last=True)# 2 use new col del df[&#39;index&#39;] # 3 del the col df.sort_index(inplace=True)# 4 tidy up . Iterating over DataFrame rows (top) . for (index, row) in df.iterrows(): # pass . Sorting . Rows values (top) . df = df.sort(df.columns[0], ascending=False) df.sort([&#39;col1&#39;, &#39;col2&#39;], inplace=True) . By row index (top) . df.sort_index(inplace=True) # sort by row df = df.sort_index(ascending=False) . Random (top) . import random as r k = 20 # pick a number selection = r.sample(range(len(df)), k) df_sample = df.iloc[selection, :] . df.take(np.random.permutation(len(df))[:3]) . Cells . Selecting . By row and column (top) . value = df.at[&#39;row&#39;, &#39;col&#39;] value = df.loc[&#39;row&#39;, &#39;col&#39;] value = df[&#39;col&#39;].at[&#39;row&#39;] # tricky . Note: .at[] fastest label based scalar lookup . By integer position (top) . value = df.iat[9, 3] # [row, col] value = df.iloc[0, 0] # [row, col] value = df.iloc[len(df)-1, len(df.columns)-1] . Slice by labels (top) . df = df.loc[&#39;row1&#39;:&#39;row3&#39;, &#39;col1&#39;:&#39;col3&#39;] . Slice by Integer Position (top) . df = df.iloc[2:4, 2:4] # subset of the df df = df.iloc[:5, :5] # top left corner s = df.iloc[5, :] # returns row as Series df = df.iloc[5:6, :] # returns row as row . By label and/or Index (top) . value = df.ix[5, &#39;col1&#39;] df = df.ix[1:5, &#39;col1&#39;:&#39;col3&#39;] . Manipulating . Setting a cell by row and column labels (top) . # pandas update df.at[&#39;row&#39;, &#39;col&#39;] = value df.loc[&#39;row&#39;, &#39;col&#39;] = value df[&#39;col&#39;].at[&#39;row&#39;] = value # tricky . Setting a cross-section by labels . df.loc[&#39;A&#39;:&#39;C&#39;, &#39;col1&#39;:&#39;col3&#39;] = np.nan df.loc[1:2,&#39;col1&#39;:&#39;col2&#39;]=np.zeros((2,2)) df.loc[1:2,&#39;A&#39;:&#39;C&#39;]=othr.loc[1:2,&#39;A&#39;:&#39;C&#39;] . Setting cell by integer position . df.iloc[0, 0] = value # [row, col] df.iat[7, 8] = value . Setting cell range by integer position . df.iloc[0:3, 0:5] = value df.iloc[1:3, 1:4] = np.ones((2, 3)) df.iloc[1:3, 1:4] = np.zeros((2, 3)) df.iloc[1:3, 1:4] = np.array([[1, 1, 1],[2, 2, 2]]) . Data wrangling . Merge Join . More examples: https://www.geeksforgeeks.org/python-pandas-merging-joining-and-concatenating/ . . (top) . Three ways to join two DataFrames: . merge (a database/SQL-like join operation) | concat (stack side by side or one on top of the other) | combine_first (splice the two together, choosing values from one over the other) | . # pandas merge # Merge on indexes df_new = pd.merge(left=df1, right=df2, how=&#39;outer&#39;, left_index=True, right_index=True) # Merge on columns df_new = pd.merge(left=df1, right=df2, how=&#39;left&#39;, left_on=&#39;col1&#39;, right_on=&#39;col2&#39;) # Join on indexes (another way of merging) df_new = df1.join(other=df2, on=&#39;col1&#39;,how=&#39;outer&#39;) df_new = df1.join(other=df2,on=[&#39;a&#39;,&#39;b&#39;],how=&#39;outer&#39;) # Simple concatenation is often the best # pandas concat df=pd.concat([df1,df2],axis=0)#top/bottom df = df1.append([df2, df3]) #top/bottom df=pd.concat([df1,df2],axis=1)#left/right # Combine_first (&lt;a href=&quot;#top&quot;&gt;top&lt;/a&gt;) df = df1.combine_first(other=df2) # multi-combine with python reduce() df = reduce(lambda x, y: x.combine_first(y), [df1, df2, df3, df4, df5]) . (top) . GroupBy . # pandas groupby # Grouping gb = df.groupby(&#39;cat&#39;) # by one columns gb = df.groupby([&#39;c1&#39;,&#39;c2&#39;]) # by 2 cols gb = df.groupby(level=0) # multi-index gb gb = df.groupby(level=[&#39;a&#39;,&#39;b&#39;]) # mi gb print(gb.groups) # Iterating groups – usually not needed # pandas groupby iterate for name, group in gb: print (name) print (group) # Selecting a group (&lt;a href=&quot;#top&quot;&gt;top&lt;/a&gt;) dfa = df.groupby(&#39;cat&#39;).get_group(&#39;a&#39;) dfb = df.groupby(&#39;cat&#39;).get_group(&#39;b&#39;) # pandas groupby aggregate # Applying an aggregating function # apply to a column ... s = df.groupby(&#39;cat&#39;)[&#39;col1&#39;].sum() s = df.groupby(&#39;cat&#39;)[&#39;col1&#39;].agg(np.sum) # apply to the every column in DataFrame s = df.groupby(&#39;cat&#39;).agg(np.sum) df_summary = df.groupby(&#39;cat&#39;).describe() df_row_1s = df.groupby(&#39;cat&#39;).head(1) # Applying multiple aggregating functions gb = df.groupby(&#39;cat&#39;) # apply multiple functions to one column dfx = gb[&#39;col2&#39;].agg([np.sum, np.mean]) # apply to multiple fns to multiple cols dfy = gb.agg({ &#39;cat&#39;: np.count_nonzero, &#39;col1&#39;: [np.sum, np.mean, np.std], &#39;col2&#39;: [np.min, np.max] }) Note: gb[&#39;col2&#39;] above is shorthand for df.groupby(&#39;cat&#39;)[&#39;col2&#39;], without the need for regrouping. # Transforming functions (&lt;a href=&quot;#top&quot;&gt;top&lt;/a&gt;) # pandas groupby function # transform to group z-scores, which have # a group mean of 0, and a std dev of 1. zscore = lambda x: (x-x.mean())/x.std() dfz = df.groupby(&#39;cat&#39;).transform(zscore) # pandas groupby fillna # replace missing data with group mean mean_r = lambda x: x.fillna(x.mean()) dfm = df.groupby(&#39;cat&#39;).transform(mean_r) # Applying filtering functions (&lt;a href=&quot;#top&quot;&gt;top&lt;/a&gt;) # select groups with more than 10 members eleven = lambda x: (len(x[&#39;col1&#39;]) &gt;= 11) df11 = df.groupby(&#39;cat&#39;).filter(eleven) # Group by a row index (non-hierarchical index) df = df.set_index(keys=&#39;cat&#39;) s = df.groupby(level=0)[&#39;col1&#39;].sum() dfg = df.groupby(level=0).sum() . (top) . Dates . # pandas timestamp # Dates and time – points and spans t = pd.Timestamp(&#39;2013-01-01&#39;) t = pd.Timestamp(&#39;2013-01-01 21:15:06&#39;) t = pd.Timestamp(&#39;2013-01-01 21:15:06.7&#39;) p = pd.Period(&#39;2013-01-01&#39;, freq=&#39;M&#39;) # pandas time series # A Series of Timestamps or Periods ts = [&#39;2015-04-01 13:17:27&#39;, &#39;2014-04-02 13:17:29&#39;] # Series of Timestamps (good) s = pd.to_datetime(pd.Series(ts)) # Series of Periods (often not so good) s = pd.Series( [pd.Period(x, freq=&#39;M&#39;) for x in ts] ) s = pd.Series(pd.PeriodIndex(ts,freq=&#39;S&#39;)) # From non-standard strings to Timestamps t = [&#39;09:08:55.7654-JAN092002&#39;, &#39;15:42:02.6589-FEB082016&#39;] s = pd.Series(pd.to_datetime(t, format=&quot;%H:%M:%S.%f-%b%d%Y&quot;)) # Dates and time – stamps and spans as indexes # pandas time periods date_strs = [&#39;2014-01-01&#39;, &#39;2014-04-01&#39;,&#39;2014-07-01&#39;, &#39;2014-10-01&#39;] dti = pd.DatetimeIndex(date_strs) pid = pd.PeriodIndex(date_strs, freq=&#39;D&#39;) pim = pd.PeriodIndex(date_strs, freq=&#39;M&#39;) piq = pd.PeriodIndex(date_strs, freq=&#39;Q&#39;) print (pid[1] - pid[0]) # 90 days print (pim[1] - pim[0]) # 3 months print (piq[1] - piq[0]) # 1 quarter time_strs = [&#39;2015-01-01 02:10:40.12345&#39;, &#39;2015-01-01 02:10:50.67890&#39;] pis = pd.PeriodIndex(time_strs, freq=&#39;U&#39;) df.index = pd.period_range(&#39;2015-01&#39;, periods=len(df), freq=&#39;M&#39;) dti = pd.to_datetime([&#39;04-01-2012&#39;], dayfirst=True) # Australian date format pi = pd.period_range(&#39;1960-01-01&#39;,&#39;2015-12-31&#39;, freq=&#39;M&#39;) # Hint: unless you are working in less than seconds, prefer PeriodIndex over DateTimeImdex. . # pandas converting times From DatetimeIndex to Python datetime objects (&lt;a href=&quot;#top&quot;&gt;top&lt;/a&gt;) dti = pd.DatetimeIndex(pd.date_range( start=&#39;1/1/2011&#39;, periods=4, freq=&#39;M&#39;)) s = Series([1,2,3,4], index=dti) na = dti.to_pydatetime() #numpy array na = s.index.to_pydatetime() #numpy array # From Timestamps to Python dates or times df[&#39;date&#39;] = [x.date() for x in df[&#39;TS&#39;]] df[&#39;time&#39;] = [x.time() for x in df[&#39;TS&#39;]] # From DatetimeIndex to PeriodIndex and back df = DataFrame(np.random.randn(20,3)) df.index = pd.date_range(&#39;2015-01-01&#39;, periods=len(df), freq=&#39;M&#39;) dfp = df.to_period(freq=&#39;M&#39;) dft = dfp.to_timestamp() # Working with a PeriodIndex (&lt;a href=&quot;#top&quot;&gt;top&lt;/a&gt;) pi = pd.period_range(&#39;1960-01&#39;,&#39;2015-12&#39;,freq=&#39;M&#39;) na = pi.values # numpy array of integers lp = pi.tolist() # python list of Periods sp = Series(pi)# pandas Series of Periods ss = Series(pi).astype(str) # S of strs ls = Series(pi).astype(str).tolist() # Get a range of Timestamps dr = pd.date_range(&#39;2013-01-01&#39;, &#39;2013-12-31&#39;, freq=&#39;D&#39;) # Error handling with dates # 1st example returns string not Timestamp t = pd.to_datetime(&#39;2014-02-30&#39;) # 2nd example returns NaT (not a time) t = pd.to_datetime(&#39;2014-02-30&#39;,coerce=True) # NaT like NaN tests True for isnull() b = pd.isnull(t) # --&gt; True # The tail of a time-series DataFrame (&lt;a href=&quot;#top&quot;&gt;top&lt;/a&gt;) df = df.last(&quot;5M&quot;) # the last five months . Upsampling and downsampling . # pandas upsample pandas downsample # upsample from quarterly to monthly pi = pd.period_range(&#39;1960Q1&#39;, periods=220, freq=&#39;Q&#39;) df = DataFrame(np.random.rand(len(pi),5), index=pi) dfm = df.resample(&#39;M&#39;, convention=&#39;end&#39;) # use ffill or bfill to fill with values # downsample from monthly to quarterly dfq = dfm.resample(&#39;Q&#39;, how=&#39;sum&#39;) . Time zones . # pandas time zones t = [&#39;2015-06-30 00:00:00&#39;,&#39;2015-12-31 00:00:00&#39;] dti = pd.to_datetime(t).tz_localize(&#39;Australia/Canberra&#39;) dti = dti.tz_convert(&#39;UTC&#39;) ts = pd.Timestamp(&#39;now&#39;, tz=&#39;Europe/London&#39;) # get a list of all time zones import pyzt for tz in pytz.all_timezones: print tz # Note: by default, Timestamps are created without timezone information. # Row selection with a time-series index # start with the play data above idx = pd.period_range(&#39;2015-01&#39;, periods=len(df), freq=&#39;M&#39;) df.index = idx february_selector = (df.index.month == 2) february_data = df[february_selector] q1_data = df[(df.index.month &gt;= 1) &amp; (df.index.month &lt;= 3)] mayornov_data = df[(df.index.month == 5) | (df.index.month == 11)] totals = df.groupby(df.index.year).sum() # The Series.dt accessor attribute t = [&#39;2012-04-14 04:06:56.307000&#39;, &#39;2011-05-14 06:14:24.457000&#39;, &#39;2010-06-14 08:23:07.520000&#39;] # a Series of time stamps s = pd.Series(pd.to_datetime(t)) print(s.dtype) # datetime64[ns] print(s.dt.second) # 56, 24, 7 print(s.dt.month) # 4, 5, 6 # a Series of time periods s = pd.Series(pd.PeriodIndex(t,freq=&#39;Q&#39;)) print(s.dtype) # datetime64[ns] print(s.dt.quarter) # 2, 2, 2 print(s.dt.year) # 2012, 2011, 2010 . Missing data . Missing data in a Series (top) . # pandas missing data series s = Series( [8,None,float(&#39;nan&#39;),np.nan]) #[8, NaN, NaN, NaN] s.isnull() #[False, True, True, True] s.notnull()#[True, False, False, False] s.fillna(0)#[8, 0, 0, 0] . # pandas missing data dataframe df = df.dropna() # drop all rows with NaN df = df.dropna(axis=1) # same for cols df=df.dropna(how=&#39;all&#39;) #drop all NaN row df=df.dropna(thresh=2) # drop 2+ NaN in r # only drop row if NaN in a specified col df = df.dropna(df[&#39;col&#39;].notnull()) . Recoding/Replacing missing data . # pandas fillna recoding replacing df.fillna(0, inplace=True) # np.nan -&gt; 0 s = df[&#39;col&#39;].fillna(0) # np.nan -&gt; 0 df = df.replace(r&#39; s+&#39;, np.nan,regex=True) # white space -&gt; np.nan # Non-finite numbers s = Series([float(&#39;inf&#39;), float(&#39;-inf&#39;),np.inf, -np.inf]) # Testing for finite numbers (&lt;a href=&quot;#top&quot;&gt;top&lt;/a&gt;) b = np.isfinite(s) . (top) . Categorical Data . # pandas categorical data s = Series([&#39;a&#39;,&#39;b&#39;,&#39;a&#39;,&#39;c&#39;,&#39;b&#39;,&#39;d&#39;,&#39;a&#39;], dtype=&#39;category&#39;) df[&#39;B&#39;] = df[&#39;A&#39;].astype(&#39;category&#39;) # Convert back to the original data type s = Series([&#39;a&#39;,&#39;b&#39;,&#39;a&#39;,&#39;c&#39;,&#39;b&#39;,&#39;d&#39;,&#39;a&#39;], dtype=&#39;category&#39;) s = s.astype(&#39;string&#39;) # Ordering, reordering and sorting s = Series(list(&#39;abc&#39;), dtype=&#39;category&#39;) print (s.cat.ordered) s=s.cat.reorder_categories([&#39;b&#39;,&#39;c&#39;,&#39;a&#39;]) s = s.sort() s.cat.ordered = False # Renaming categories (&lt;a href=&quot;#top&quot;&gt;top&lt;/a&gt;) s = Series(list(&#39;abc&#39;), dtype=&#39;category&#39;) s.cat.categories = [1, 2, 3] # in place s = s.cat.rename_categories([4,5,6]) # using a comprehension ... s.cat.categories = [&#39;Group &#39; + str(i) for i in s.cat.categories] # Adding new categories (&lt;a href=&quot;#top&quot;&gt;top&lt;/a&gt;) s = s.cat.add_categories([4]) # Removing categories (&lt;a href=&quot;#top&quot;&gt;top&lt;/a&gt;) s = s.cat.remove_categories([4]) s.cat.remove_unused_categories() #inplace . (top) . Manipulations and Cleaning . Conversions . # pandas convert to numeric ## errors=&#39;ignore&#39;` ## `errors=&#39;coerce` convert to `np.nan` ## mess up data invoices.loc[45612,&#39;Meal Price&#39;] = &#39;I am causing trouble&#39; invoices.loc[35612,&#39;Meal Price&#39;] = &#39;Me too&#39; # check if conversion worked invoices[&#39;Meal Price&#39;].apply(lambda x:type(x)).value_counts() **OUT: &lt;class &#39;int&#39;&gt; 49972 &lt;class &#39;str&#39;&gt; 2 # identify validating lines invoices[&#39;Meal Price&#39;][invoices[&#39;Meal Price&#39;].apply( lambda x: isinstance(x,str) )] . # convert messy numerical data ## convert the offending values into np.nan** invoices[&#39;Meal Price&#39;] = pd.to_numeric(invoices[&#39;Meal Price&#39;],errors=&#39;coerce&#39;) ## fill np.nan with the median of the data** invoices[&#39;Meal Price&#39;] = invoices[&#39;Meal Price&#39;].fillna(invoices[&#39;Meal Price&#39;].median()) ## convert the column into integer** invoices[&#39;Meal Price&#39;].astype(int) . # pandas convert to datetime to_datetime print(pd.to_datetime(&#39;2019-8-1&#39;)) print(pd.to_datetime(&#39;2019/8/1&#39;)) print(pd.to_datetime(&#39;8/1/2019&#39;)) print(pd.to_datetime(&#39;Aug, 1 2019&#39;)) print(pd.to_datetime(&#39;Aug - 1 2019&#39;)) print(pd.to_datetime(&#39;August - 1 2019&#39;)) print(pd.to_datetime(&#39;2019, August - 1&#39;)) print(pd.to_datetime(&#39;20190108&#39;)) . source: https://towardsdatascience.com/learn-advanced-features-for-pythons-main-data-analysis-library-in-20-minutes-d0eedd90d086 . Method chaining . https://towardsdatascience.com/the-unreasonable-effectiveness-of-method-chaining-in-pandas-15c2109e3c69 R to python: gist . # method chaining def csnap(df, fn=lambda x: x.shape, msg=None): &quot;&quot;&quot; Custom Help function to print things in method chaining. Returns back the df to further use in chaining. &quot;&quot;&quot; if msg: print(msg) display(fn(df)) return df ( wine.pipe(csnap) .rename(columns={&quot;color_intensity&quot;: &quot;ci&quot;}) .assign(color_filter=lambda x: np.where((x.hue &gt; 1) &amp; (x.ci &gt; 7), 1, 0)) .pipe(csnap) .query(&quot;alcohol &gt; 14&quot;) .pipe(csnap, lambda df: df.head(), msg=&quot;After&quot;) .sort_values(&quot;alcohol&quot;, ascending=False) .reset_index(drop=True) .loc[:, [&quot;alcohol&quot;, &quot;ci&quot;, &quot;hue&quot;]] .pipe(csnap, lambda x: x.sample(5)) ) . good explanation: https://tomaugspurger.github.io/method-chaining.html . assign (0.16.0): For adding new columns to a DataFrame in a chain (inspired by dplyr’s mutate) | pipe (0.16.2): For including user-defined methods in method chains. | rename (0.18.0): For altering axis names (in additional to changing the actual labels as before). | Window methods (0.18): Took the top-level pd.rolling_* and pd.expanding_* functions and made them NDFrame methods with a groupby-like API. | Resample (0.18.0) Added a new groupby-like API | .where/mask/Indexers accept Callables (0.18.1): In the next release you’ll be able to pass a callable to the indexing methods, to be evaluated within the DataFrame’s context (like .query, but with code instead of strings). | . https://www.quora.com/I-love-the-flexibility-of-pandas-dataframes-but-I-feel-like-they-can-make-code-harder-to-read-and-maintain-What-are-some-pandas-best-practices-that-address-this-issue . Tidyverse vs pandas: link . (top) . Binning . # binning pd.cut(np.array([1, 7, 5, 4, 6, 3]), 3, labels=[&quot;bad&quot;, &quot;medium&quot;, &quot;good&quot;]) [bad, good, medium, medium, good, bad] Categories (3, object): [bad &lt; medium &lt; good] # binning into custom intervals bins = [0, 1, 5, 10, 25, 50, 100] labels = [1,2,3,4,5,6] df[&#39;binned&#39;] = pd.cut(df[&#39;percentage&#39;], bins=bins, labels=labels) print (df) percentage binned 0 46.50 5 1 44.20 5 2 100.00 6 3 42.12 5 . source . Clipping (top) . # removing outlier df.clip(lower=pd.Series({&#39;A&#39;: 2.5, &#39;B&#39;: 4.5}), axis=1) Outlier removal python q = df[&quot;col&quot;].quantile(0.99) df[df[&quot;col&quot;] &lt; q] #or df = pd.DataFrame(np.random.randn(100, 3)) from scipy import stats df[(np.abs(stats.zscore(df)) &lt; 3).all(axis=1)] . source . df[&#39;Date of Publication&#39;] = pd.to_numeric(extr) # np.where df[&#39;Place of Publication&#39;] = np.where(london, &#39;London&#39;, np.where(oxford, &#39;Oxford&#39;, pub.str.replace(&#39;-&#39;, &#39; &#39;))) # 1929 1839, 38-54 # 2836 [1897?] regex = r&#39;^( d{4})&#39; extr = df[&#39;Date of Publication&#39;].str.extract(r&#39;^( d{4})&#39;, expand=False) # columns to ditionary master_dict = dict(df.drop_duplicates(subset=&quot;term&quot;)[[&quot;term&quot;,&quot;uid&quot;]].values.tolist()) . pivoting table https://stackoverflow.com/questions/47152691/how-to-pivot-a-dataframe . replace with map . d = {&#39;apple&#39;: 1, &#39;peach&#39;: 6, &#39;watermelon&#39;: 4, &#39;grapes&#39;: 5, &#39;orange&#39;: 2,&#39;banana&#39;: 3} df[&quot;fruit_tag&quot;] = df[&quot;fruit_tag&quot;].map(d) . regex matching groups https://stackoverflow.com/questions/2554185/match-groups-in-python . import re mult = re.compile(&#39;(two|2) (?P&lt;race&gt;[a-z]+) (?P&lt;gender&gt;(?:fe)?male)s&#39;) s = &#39;two hispanic males, 2 hispanic females&#39; mult.sub(r&#39; g&lt;race&gt; g&lt;gender&gt;, g&lt;race&gt; g&lt;gender&gt;&#39;, s) # &#39;hispanic male, hispanic male, hispanic female, hispanic female&#39; . source . (top) . test if type is string is equal . isinstance(s, str) . apply function to column . df[&#39;a&#39;] = df[&#39;a&#39;].apply(lambda x: x + 1) . exploding a column . df = pd.DataFrame([{&#39;var1&#39;: &#39;a,b,c&#39;, &#39;var2&#39;: 1}, {&#39;var1&#39;: &#39;d,e,f&#39;, &#39;var2&#39;: 2}]) df.assign(var1=df.var1.str.split(&#39;,&#39;)).explode(&#39;var1&#39;) . . (top) . Performance . Reshaping dataframe . The similarity between melt and stack: blog post . . sorting dataframe . df = pd.read_csv(&quot;data/347136217_T_ONTIME.csv&quot;) delays = df[&#39;DEP_DELAY&#39;] # Select the 5 largest delays delays.nlargest(5).sort_values() . %timeit delays.sort_values().tail(5) 31 ms ± 1.05 ms per loop (mean ± std. dev. of 7 runs, 10 loops each) %timeit delays.nlargest(5).sort_values() 7.87 ms ± 113 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) . check memory usage: . c = s.astype(&#39;category&#39;) print(&#39;{:0.2f} KB&#39;.format(c.memory_usage(index=False) / 1000)) . (top) . Concat vs. append . source . fast append via list of dictionaries: . rows_list = [] for row in input_rows: dict1 = {} # get input row in dictionary format # key = col_name dict1.update(blah..) rows_list.append(dict1) df = pd.DataFrame(rows_list) . source: link . alternatives . #Append def f1(): result = df for i in range(9): result = result.append(df) return result # Concat def f2(): result = [] for i in range(10): result.append(df) return pd.concat(result) In [101]: %timeit f1() 1 loops, best of 3: 1.66 s per loop In [102]: %timeit f2() 1 loops, best of 3: 220 ms per loop . timings = (pd.DataFrame({&quot;Append&quot;: t_append, &quot;Concat&quot;: t_concat}) .stack() .reset_index() .rename(columns={0: &#39;Time (s)&#39;, &#39;level_1&#39;: &#39;Method&#39;})) timings.head() . (top) . Dataframe: iterate rows . Useful links . how-to-iterate-over-rows-in-a-dataframe-in-pandas: link | how-to-make-your-pandas-loop-71-803-times-faster: link | example of bringing down runtime: iterrows, iloc, get_value, apply: link | complex example using haversine_looping: link, jupyter notebook | different-ways-to-iterate-over-rows-in-a-pandas-dataframe-performance-comparison: link | pandas performance tweaks: cython, using numba | . . Vectorization | Cython routines | List Comprehensions (vanilla for loop) | DataFrame.apply(): i) Reductions that can be performed in cython, ii) Iteration in python space | DataFrame.itertuples() and iteritems() | DataFrame.iterrows() | (top) . Profiling book chapter from jakevdp: link . %timeit sum(range(100)) # single line %timeit np.arange(4)[pd.Series([1, 2, 3])] %timeit np.arange(4)[pd.Series([1, 2, 3]).values] 111 µs ± 2.25 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each) 61.1 µs ± 2.7 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each) %%timeit # full cell total = 0 for i in range(1000): for j in range(1000): total += i * (-1) ** j # profiling def sum_of_lists(N): total = 0 for i in range(5): L = [j ^ (j &gt;&gt; i) for j in range(N)] total += sum(L) return total %prun sum_of_lists(1000000) %load_ext line_profiler %lprun -f sum_of_lists sum_of_lists(5000) # memory usage %load_ext memory_profiler %memit sum_of_lists(1000000) . performance plots (notebook link): . import perfplot import pandas as pd import numpy as np perfplot.show( setup=lambda n: pd.DataFrame(np.random.choice(1000, (n, 2)), columns=[&#39;A&#39;,&#39;B&#39;]), kernels=[ lambda df: df[df.A != df.B], lambda df: df.query(&#39;A != B&#39;), lambda df: df[[x != y for x, y in zip(df.A, df.B)]] ], labels=[&#39;vectorized !=&#39;, &#39;query (numexpr)&#39;, &#39;list comp&#39;], n_range=[2**k for k in range(0, 15)], xlabel=&#39;N&#39; ) . (top) . list comprehension . # iterating over one column - `f` is some function that processes your data result = [f(x) for x in df[&#39;col&#39;]] # iterating over two columns, use `zip` result = [f(x, y) for x, y in zip(df[&#39;col1&#39;], df[&#39;col2&#39;])] # iterating over multiple columns result = [f(row[0], ..., row[n]) for row in df[[&#39;col1&#39;, ...,&#39;coln&#39;]].values] . Further tipps . Do numerical calculations with NumPy functions. They are two orders of magnitude faster than Python’s built-in tools. | Of Python’s built-in tools, list comprehension is faster than map() , which is significantly faster than for. | For deeply recursive algorithms, loops are more efficient than recursive function calls. | You cannot replace recursive loops with map(), list comprehension, or a NumPy function. | “Dumb” code (broken down into elementary operations) is the slowest. Use built-in functions and tools. | . source: example code, link . (top) . Parallel data structures . https://towardsdatascience.com/make-your-own-super-pandas-using-multiproc-1c04f41944a1 | https://github.com/modin-project/modin | https://github.com/jmcarpenter2/swifter | datatable blog post | vaex: talk, blog,github | . https://learning.oreilly.com/library/view/python-high-performance/9781787282896/ . get all combinations from two columns . tuples = [tuple(x) for x in dm_bmindex_df_without_index_df[[&#39;trial&#39;, &#39;biomarker_name&#39;]].values] . (top) . Jupyter notebooks . jupyter notebook best practices (link): structure your notebook, automate jupyter execution: link . Extensions: general, | snippets extension: link | convert notebook. post-save hook: gist | jupyter theme github link: jt -t grade3 -fs 95 -altp -tfs 11 -nfs 115 -cellw 88% -T . From jupyter notebooks to standalone apps (Voila): github, blog (example github PR) . | . jupyter lab: Shortcut to run single command: stackoverflow . Notebooks in production . directory structure, layout, workflow: blog post also: cookiecutter . workflow . . (top) . Directory structure . raw - Contains the unedited csv and Excel files used as the source for analysis. | interim - Used if there is a multi-step manipulation. This is a scratch location and not always needed but helpful to have in place so directories do not get cluttered or as a temp location form troubleshooting issues. | processed - In many cases, I read in multiple files, clean them up and save them to a new location in a binary format. This streamlined format makes it easier to read in larger files later in the processing pipeline. | . (top) . Further link . how netflix runs notebooks: scheduling, integration testing: link . jupyter notebook template . header section . A good name for the notebook (as described above) | A summary header that describes the project | Free form description of the business reason for this notebook. I like to include names, dates and snippets of emails to make sure I remember the context. | A list of people/systems where the data originated. | I include a simple change log. I find it helpful to record when I started and any major changes along the way. I do not update it with every single change but having some date history is very beneficial. | . (top) . Orchestration . https://taskfile.dev/#/ . jupyter code snippets . # jupyter notebook --generate-config jupyter notebook --generate-config # start in screen session screen -d -m -S JUPYTER jupyter notebook --ip 0.0.0.0 --port 8889 --no-browser --NotebookApp.token=&#39;&#39; # install packages in jupyter !pip install package-name # environment variables %%bash which python # reset/set password jupyter notebook password # show all running notebooks jupyter notebook list # (Can be useful to get a hash for a notebook) . append to path . from os.path import dirname sys.path.append(dirname(__file__)) . hide warnings . import warnings warnings.filterwarnings(&#39;ignore&#39;) warnings.filterwarnings(action=&#39;once&#39;) . tqdm (top) . from tqdm import tqdm for i in tqdm(range(10000)): . qqrid (top) . import qqrid qqrid_widget = qqrid.show_grid(df, show_toolbar=True) qqrid_widget . print all numpy . import numpy numpy.set_printoptions(threshold=numpy.nan) . Debugging: . ipdb . # debug ipdb from IPython.core.debugger import set_trace def select_condition(tmp): set_trace() . Pixie debugger . # built-in profiler %prun -l 4 estimate_and_update(100) # line by line profiling pip install line_profiler %load_ext line_profiler %lprun -f sum_of_lists sum_of_lists(5000) # memory usage pip install memory_profiler %load_ext memory_profiler %memit sum_of_lists(1000000) . source: Timing and profiling . add tags to jupyterlab: https://github.com/jupyterlab/jupyterlab/issues/4100 . { &quot;tags&quot;: [ &quot;to_remove&quot; ], &quot;slideshow&quot;: { &quot;slide_type&quot;: &quot;fragment&quot; } } . removing tags: https://groups.google.com/forum/#!topic/jupyter/W2M_nLbboj4 . (top) . Timing and Profiling . https://jakevdp.github.io/PythonDataScienceHandbook/01.07-timing-and-profiling.html . test code . test driven development in jupyter notebook . asserts . def multiplyByTwo(x): return x * 3 assert multiplyByTwo(2) == 4, &quot;2 multiplied by 2 should be equal 4&quot; # test file size assert os.path.getsize(bm_index_master_file) &gt; 150000000, &#39;Output file size should be &gt; 150Mb&#39; # assert is nan assert np.isnan(ret_none), f&quot;Can&#39;t deal with &#39;None values&#39;: {ret_none} == {np.nan}&quot; . Production-ready notebooks: link If tqdm doesnt work: install ipywidgets Hbox full: link . Qgrid . Qgrid readme . qgrid.show_grid(e_tpatt_df, grid_options={&#39;forceFitColumns&#39;: False, &#39;defaultColumnWidth&#39;: 100}) . Debugging conda . # conda show install versions import sys print(sys.path) or import sys, fastai print(sys.modules[&#39;fastai&#39;]) . Running Jupyter . jupyter notebook --browser=false &amp;&gt; /dev/null &amp; --matplotlib inline --port=9777 --browser=false # Check GPU is working GPU working from tensorflow.python.client import device_lib def get_available_devices(): local_device_protos = device_lib.list_local_devices() return [x.name for x in local_device_protos] print(get_available_devices()) . (top) . installing kernels . # conda install kernel source activate &lt;ANACONDA_ENVIRONMENT_NAME&gt; pip install ipykernel python -m ipykernel install --user or source activate myenv python -m ipykernel install --user --name myenv --display-name &quot;Python (myenv)&quot; . source stackoverflow . (top) . ## . unsorted . # use dictionary to count list &gt;&gt;&gt; from collections import Counter &gt;&gt;&gt; Counter([&#39;apple&#39;,&#39;red&#39;,&#39;apple&#39;,&#39;red&#39;,&#39;red&#39;,&#39;pear&#39;]) Counter({&#39;red&#39;: 3, &#39;apple&#39;: 2, &#39;pear&#39;: 1}) . # dictionary keys to list list(dict.keys()) . # dictionary remove nan # if nan in keys clean_dict = filter(lambda k: not isnan(k), my_dict) # if nan in values clean_dict = filter(lambda k: not isnan(my_dict[k]), my_dict) . # list remove nan cleanedList = [x for x in countries if str(x) != &#39;nan&#39;] . # pandas convert all columns to lowercase df.apply(lambda x: x.astype(str).str.lower()) . # pandas set difference tow columns # source: https://stackoverflow.com/questions/18180763/set-difference-for-pandas from pandas import DataFrame df1 = DataFrame({&#39;col1&#39;:[1,2,3], &#39;col2&#39;:[2,3,4]}) df2 = DataFrame({&#39;col1&#39;:[4,2,5], &#39;col2&#39;:[6,3,5]}) print df2[~df2.isin(df1).all(1)] print df2[(df2!=df1)].dropna(how=&#39;all&#39;) print df2[~(df2==df1)].dropna(how=&#39;all&#39;) # union print(&quot;Union :&quot;, A | B) # intersection print(&quot;Intersection :&quot;, A &amp; B) # difference print(&quot;Difference :&quot;, A - B) # symmetric difference print(&quot;Symmetric difference :&quot;, A ^ B) . # pandas value counts to dataframe df = value_counts.rename_axis(&#39;unique_values&#39;).reset_index(name=&#39;counts&#39;) . # python dictionary get first key list(tree_number_dict.keys())[0] . # pandas dataframe get cell value by condition function(df.loc[df[&#39;condition&#39;].isna(),&#39;condition&#39;].values[0],1) . # dataframe drop duplicates keep first df = df.drop_duplicates(cols=&#39;index&#39;,take_last=True)# 2 use new col . (top) .",
            "url": "/blog/python/cheat%20sheet/2020/03/23/data-science-snippets.html",
            "relUrl": "/python/cheat%20sheet/2020/03/23/data-science-snippets.html",
            "date": " • Mar 23, 2020"
        }
        
    
  
    
  
    
        ,"post6": {
            "title": "Python plotting libraries",
            "content": "How to choose a chart . image source: experception.net . good source . source . Individual charts . overview of charts explained: https://datavizcatalogue.com/index.html . boxplot: https://towardsdatascience.com/understanding-boxplots-5e2df7bcbd51 . Regression . https://seaborn.pydata.org/tutorial/regression.html . Preliminaries . import matplotlib.pyplot as plt import pandas as pd . Images . Image from Numpy array . from PIL import Image j = Image.fromarray(img, mode=&#39;RGB&#39;) #print(&#39;saving file &#39;,outfile) j.save(outfile) . Plotting . List of links to plotting resources . Seaborn . Seaborn cheat sheet: https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Python_Seaborn_Cheat_Sheet.pdf Interactive testing of plots: https://www.datacamp.com/community/tutorials/seaborn-python-tutorial . Bokeh interactive app (standalone) . blog post, nbviewer plotly (dash) example: kigadataset Graz (nbviewer) . Plotly . express . main page,walkthrough.ipynb . Great tutorial using plot.ly: link and plotly editor for jupyter lab here . Altair . gallery use cases: https://covid19dashboards.com/, . vega: examples, interactive_seattle_weather, jupyter plugin (not needed for jupyterlab) . (Back to top) . Useful code snippets . to be added . Dashboards . Streamlit . main page, https://www.streamlit.io/gallery . Visualize any Data Easily, from Notebooks to Dashboards | Scipy 2019 Tutorial | James Bednar: link | . (Back to top) .",
            "url": "/blog/python/plotting/2020/03/03/python-plotting-libraries.html",
            "relUrl": "/python/plotting/2020/03/03/python-plotting-libraries.html",
            "date": " • Mar 3, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Python cheat sheet",
            "content": "Preliminaries . import numpy as np import matplotlib.pyplot as plt import pandas as pd from pandas import DataFrame, Series . (Back to top) . It is also possible to write Python code which is compatible with Python 2.7 and 3.x at the same time, using Python future imports. future imports allow you to write Python 3 code that will run on Python 2, so check out the Python 3 tutorial. . comments start with a number symbol. . &quot;&quot;&quot; Multiline strings can be written using three &quot;s, and are often used as comments &quot;&quot;&quot; . (Back to top) . . Primitive Datatypes and Operators . # You have numbers 3 # =&gt; 3 . Math . # Math is what you would expect 1 + 1 # =&gt; 2 8 - 1 # =&gt; 7 10 * 2 # =&gt; 20 35 / 5 # =&gt; 7 # Division is a bit tricky. It is integer division and floors the results automatically. 5 / 2 # =&gt; 2 # To fix division we need to learn about floats. 2.0 # This is a float 11.0 / 4.0 # =&gt; 2.75 ahhh...much better # Result of integer division truncated down both for positive and negative. 5 // 3 # =&gt; 1 5.0 // 3.0 # =&gt; 1.0 # works on floats too -5 // 3 # =&gt; -2 -5.0 // 3.0 # =&gt; -2.0 # Note that we can also import division module(Section 6 Modules) to carry out normal division with just one &#39;/&#39;. from __future__ import division 11/4 # =&gt; 2.75 ...normal division 11//4 # =&gt; 2 ...floored division # Modulo operation 7 % 3 # =&gt; 1 # Exponentiation (x to the yth power) python 2**4 # =&gt; 16 . Logic . # Enforce precedence with parentheses (1 + 3) * 2 # =&gt; 8 # Boolean Operators Note &quot;and&quot; and &quot;or&quot; are case-sensitive True and False #=&gt; False False or True #=&gt; True # Note using Bool operators with ints 0 and 2 #=&gt; 0 -5 or 0 #=&gt; -5 0 == False #=&gt; True 2 == True #=&gt; False 1 == True #=&gt; True # negate with not not True # =&gt; False not False # =&gt; True # Equality is == 1 == 1 # =&gt; True 2 == 1 # =&gt; False # Inequality is != 1 != 1 # =&gt; False 2 != 1 # =&gt; True # More comparisons 1 &lt; 10 # =&gt; True 1 &gt; 10 # =&gt; False 2 &lt;= 2 # =&gt; True 2 &gt;= 2 # =&gt; True # Comparisons can be chained! 1 &lt; 2 &lt; 3 # =&gt; True 2 &lt; 3 &lt; 2 # =&gt; False . (Back to top) . . Strings . # Strings are created with &quot; or &#39; python &quot;This is a string.&quot; &#39;This is also a string.&#39; # Strings can be added too! &quot;Hello &quot; + &quot;world!&quot; # =&gt; &quot;Hello world!&quot; # Strings can be added without using &#39;+&#39; &quot;Hello &quot; &quot;world!&quot; # =&gt; &quot;Hello world!&quot; # ... or multiplied &quot;Hello&quot; * 3 # =&gt; &quot;HelloHelloHello&quot; # A string can be treated like a list of characterspython &quot;This is a string&quot;[0] # =&gt; &#39;T&#39; # String formatting with % Even though the % string operator will be deprecated on Python 3.1 and removed later at some time, it may still be good to know how it works. x = &#39;apple&#39; y = &#39;lemon&#39; z = &quot;The items in the basket are %s and %s&quot; % (x,y) # A newer way to format strings is the format method. This method is the preferred way &quot;{} is a {}&quot;.format(&quot;This&quot;, &quot;placeholder&quot;) &quot;{0} can be {1}&quot;.format(&quot;strings&quot;, &quot;formatted&quot;) # You can use keywords if you don&#39;t want to count. &quot;{name} wants to eat {food}&quot;.format(name=&quot;Bob&quot;, food=&quot;lasagna&quot;) . (Back to top) . . Object . # None is an object None # =&gt; None # Don&#39;t use the equality &quot;==&quot; symbol to compare objects to None # Use &quot;is&quot; instead &quot;etc&quot; is None # =&gt; False None is None # =&gt; True # The &#39;is&#39; operator tests for object identity. This isn&#39;t * very useful when dealing with primitive values, but is * very useful when dealing with objects. ## Any object can be used in a Boolean context. The following values are considered falsey: * - None * - zero of any numeric type (e.g., 0, 0L, 0.0, 0j) * - empty sequences (e.g., &#39;&#39;, (), []) * - empty containers (e.g., {}, set()) * - instances of user-defined classes meeting certain conditions * see: https://docs.python.org/2/reference/datamodel.html#object.__nonzero__ All other values are truthy (using the bool() function on them returns True). bool(0) # =&gt; False bool(&quot;&quot;) # =&gt; False . (Back to top) . . Variables and Collections . # Python has a print statement print &quot;I&#39;m Python. Nice to meet you!&quot; # =&gt; I&#39;m Python. Nice to meet you! # Simple way to get input data from console input_string_var = raw_input(&quot;Enter some data: &quot;) # Returns the data as a string input_var = input(&quot;Enter some data: &quot;) # Evaluates the data as python code # Warning: Caution is recommended for input() method usage&lt;br&gt; # Note: In python 3, input() is deprecated and raw_input() is renamed to input() . Variables . # No need to declare variables before assigning to them. some_var = 5 # Convention is to use lower_case_with_underscores some_var # =&gt; 5 # Accessing a previously unassigned variable is an exception. some_other_var # Raises a name error # ternary operator &quot;yahoo!&quot; if 3 &gt; 2 else 2 # =&gt; &quot;yahoo!&quot; . (Back to top) . . Lists . # Lists store sequences li = [] # You can start with a prefilled list other_li = [4, 5, 6] # Add stuff to the end of a list with append li.append(1) # li is now [1] li.append(2) # li is now [1, 2] li.append(4) # li is now [1, 2, 4] li.append(3) # li is now [1, 2, 4, 3] # Remove from the end with pop li.pop() # =&gt; 3 and li is now [1, 2, 4] # Let&#39;s put it back li.append(3) # li is now [1, 2, 4, 3] again. # Access a list like you would any array li[0] # =&gt; 1 # Assign new values to indexes that have already been initialized with = li[0] = 42 li[0] # =&gt; 42 li[0] = 1 # Note: setting it back to the original value # Look at the last element li[-1] # =&gt; 3 # Looking out of bounds is an IndexError li[4] # Raises an IndexError # You can look at ranges with slice syntax. (It&#39;s a closed/open range for you mathy types.) li[1:3] # =&gt; [2, 4] # Omit the beginning li[2:] # =&gt; [4, 3] # Omit the end li[:3] # =&gt; [1, 2, 4] # Select every second entry li[::2] # =&gt;[1, 4] # Reverse a copy of the list li[::-1] # =&gt; [3, 4, 2, 1] # Use any combination of these to make advanced slices li[start:end:step] # Remove arbitrary elements from a list with &quot;del&quot; del li[2] # li is now [1, 2, 3] # You can add lists li + other_li # =&gt; [1, 2, 3, 4, 5, 6] # Note: values for li and for other_li are not modified. # Concatenate lists with &quot;extend()&quot; li.extend(other_li) # Now li is [1, 2, 3, 4, 5, 6] # Remove first occurrence of a value li.remove(2) # li is now [1, 3, 4, 5, 6] li.remove(2) # Raises a ValueError as 2 is not in the list # Insert an element at a specific index li.insert(1, 2) # li is now [1, 2, 3, 4, 5, 6] again # Get the index of the first item found li.index(2) # =&gt; 1 li.index(7) # Raises a ValueError as 7 is not in the list # Check for existence in a list with &quot;in&quot; 1 in li # =&gt; True # Examine the length with &quot;len()&quot; len(li) # =&gt; 6 . (Back to top) . . Tuples . # Tuples are like lists but are immutable. tup = (1, 2, 3) tup[0] # =&gt; 1 tup[0] = 3 # Raises a TypeError # You can do all those list thingies on tuples too len(tup) # =&gt; 3 tup + (4, 5, 6) # =&gt; (1, 2, 3, 4, 5, 6) tup[:2] # =&gt; (1, 2) 2 in tup # =&gt; True # You can unpack tuples (or lists) into variables a, b, c = (1, 2, 3) # a is now 1, b is now 2 and c is now 3 d, e, f = 4, 5, 6 # you can leave out the parentheses # Tuples are created by default if you leave out the parentheses g = 4, 5, 6 # =&gt; (4, 5, 6) # Now look how easy it is to swap two values e, d = d, e # d is now 5 and e is now 4 . (Back to top) . Dictionary . # Dictionaries store mappings empty_dict = {} filled_dict = {&quot;one&quot;: 1, &quot;two&quot;: 2, &quot;three&quot;: 3} # Look up values with [] filled_dict[&quot;one&quot;] # =&gt; 1 # get keys filled_dict.keys() # =&gt; [&quot;three&quot;, &quot;two&quot;, &quot;one&quot;] # Note - Dictionary key ordering is not guaranteed. # Get all values as a list with &quot;values()&quot; filled_dict.values() # =&gt; [3, 2, 1] # Iterate over keys, values for word, count in sorted(count_words(filename).items()): print word, count ### join dictionary keys into string &#39;&lt;br/&gt;&#39;.join([&#39;%s:: %s&#39; % (key, value) for (key, value) in d.items()]) ## Lookup ### Check for existence of keys in a dictionary with &quot;in&quot; &quot;one&quot; in filled_dict # =&gt; True 1 in filled_dict # =&gt; False # Looking up a non-existing key is a KeyError filled_dict[&quot;four&quot;] # KeyError # Use &quot;get()&quot; method to avoid the KeyError filled_dict.get(&quot;one&quot;) # =&gt; 1 filled_dict.get(&quot;four&quot;) # =&gt; None # The get method supports a default argument when the value is missing filled_dict.get(&quot;one&quot;, 4) # =&gt; 1 filled_dict.get(&quot;four&quot;, 4) # =&gt; 4 note that filled_dict.get(&quot;four&quot;) is still =&gt; None (get doesn&#39;t set the value in the dictionary) ## Set value # set the value of a key with a syntax similar to lists filled_dict[&quot;four&quot;] = 4 # now, filled_dict[&quot;four&quot;] =&gt; 4 # &quot;setdefault()&quot; inserts into a dictionary only if the given key isn&#39;t present filled_dict.setdefault(&quot;five&quot;, 5) # filled_dict[&quot;five&quot;] is set to 5 filled_dict.setdefault(&quot;five&quot;, 6) # filled_dict[&quot;five&quot;] is still 5 . (Back to top) . . Sets . # Sets store ... well sets (which are like lists but can contain no duplicates) empty_set = set() ## Initialize a &quot;set()&quot; with a bunch of values some_set = set([1, 2, 2, 3, 4]) # some_set is now set([1, 2, 3, 4]) # order is not guaranteed, even though it may sometimes look sorted another_set = set([4, 3, 2, 2, 1]) # another_set is now set([1, 2, 3, 4]) ## Since Python 2.7, {} can be used to declare a set filled_set = {1, 2, 2, 3, 4} # =&gt; {1, 2, 3, 4} # Add more items to a set filled_set.add(5) # filled_set is now {1, 2, 3, 4, 5} ## Intersection other_set = {3, 4, 5, 6} filled_set &amp; other_set # =&gt; {3, 4, 5} ## Union with | filled_set | other_set # =&gt; {1, 2, 3, 4, 5, 6} # Set difference with - {1, 2, 3, 4} - {2, 3, 5} # =&gt; {1, 4} ## set symmetric difference with ^ {1, 2, 3, 4} ^ {2, 3, 5} # =&gt; {1, 4, 5} ## Check if set on the left is a superset of set on the right {1, 2} &gt;= {1, 2, 3} # =&gt; False ## Check if set on the left is a subset of set on the right {1, 2} &lt;= {1, 2, 3} # =&gt; True ## Check for existence in a set with in 2 in filled_set # =&gt; True 10 in filled_set # =&gt; False . (Back to top) . . Control Flow . # Let&#39;s just make a variable some_var = 5 # If ## Here is an if statement. Indentation is significant in python! if some_var &gt; 10: print &quot;some_var is totally bigger than 10.&quot; elif some_var &lt; 10: # This elif clause is optional. print &quot;some_var is smaller than 10.&quot; else: # This is optional too. print &quot;some_var is indeed 10.&quot; . ## For &quot;&quot;&quot; For loops iterate over lists prints: dog is a mammal cat is a mammal mouse is a mammal &quot;&quot;&quot; for animal in [&quot;dog&quot;, &quot;cat&quot;, &quot;mouse&quot;]: # You can use {0} to interpolate formatted strings. (See above.) print &quot;{0} is a mammal&quot;.format(animal) &quot;&quot;&quot; . ## Range for i in range(4): print i &quot;&quot;&quot; &quot;range(lower, upper)&quot; returns a list of numbers from the lower number to the upper number prints: 4 5 6 7 &quot;&quot;&quot; for i in range(4, 8): print i . ## While While loops go until a condition is no longer met. prints: 0 1 2 3 &quot;&quot;&quot; x = 0 while x &lt; 4: print x x += 1 # Shorthand for x = x + 1 . Exceptions . # Handle exceptions with a try/except block try: # Use &quot;raise&quot; to raise an error raise IndexError(&quot;This is an index error&quot;) except IndexError as e: pass # Pass is just a no-op. Usually you would do recovery here. except (TypeError, NameError): pass # Multiple exceptions can be handled together, if required. else: # Optional clause to the try/except block. Must follow all except blocks print &quot;All good!&quot; # Runs only if the code in try raises no exceptions finally: # Execute under all circumstances print &quot;We can clean up resources here&quot; . # Instead of try/finally to cleanup resources you can use a with statement with open(&quot;myfile.txt&quot;) as f: for line in f: print line . Functions . ## Use &quot;def&quot; to create new functions def add(x, y): print &quot;x is {0} and y is {1}&quot;.format(x, y) return x + y # Return values with a return statement #Calling functions with parameters add(5, 6) # =&gt; prints out &quot;x is 5 and y is 6&quot; and returns 11 # Another way to call functions is with keyword arguments add(y=6, x=5) # Keyword arguments can arrive in any order. # You can define functions that take a variable number of positional args, which will be interpreted as a tuple by using * def varargs(*args): return args varargs(1, 2, 3) # =&gt; (1, 2, 3) # You can define functions that take a variable number of keyword args, as well, which will be interpreted as a dict by using ** def keyword_args(**kwargs): return kwargs keyword_args(big=&quot;foot&quot;, loch=&quot;ness&quot;) # =&gt; {&quot;big&quot;: &quot;foot&quot;, &quot;loch&quot;: &quot;ness&quot;} ## You can do both at once, if you like def all_the_args(*args, **kwargs): print args print kwargs &quot;&quot;&quot; all_the_args(1, 2, a=3, b=4) prints: (1, 2) {&quot;a&quot;: 3, &quot;b&quot;: 4} &quot;&quot;&quot; . ## When calling functions, you can do the opposite of args/kwargs! Use * to expand positional args and use ** to expand keyword args. args = (1, 2, 3, 4) kwargs = {&quot;a&quot;: 3, &quot;b&quot;: 4} all_the_args(*args) # equivalent to foo(1, 2, 3, 4) all_the_args(**kwargs) # equivalent to foo(a=3, b=4) all_the_args(*args, **kwargs) # equivalent to foo(1, 2, 3, 4, a=3, b=4) # you can pass args and kwargs along to other functions that take args/kwargs by expanding them with * and ** respectively def pass_all_the_args(*args, **kwargs): all_the_args(*args, **kwargs) print varargs(*args) print keyword_args(**kwargs) . Function Scope . x = 5 def set_x(num): # Local var x not the same as global variable x x = num # =&gt; 43 print x # =&gt; 43 def set_global_x(num): global x print x # =&gt; 5 x = num # global var x is now set to 6 print x # =&gt; 6 set_x(43) set_global_x(6) . Python has first class functions . def create_adder(x): def adder(y): return x + y return adder add_10 = create_adder(10) add_10(3) # =&gt; 13 . (Back to top) . . There are also anonymous functions . (lambda x: x &gt; 2)(3) # =&gt; True (lambda x, y: x ** 2 + y ** 2)(2, 1) # =&gt; 5 # There are built-in higher order functions map(add_10, [1, 2, 3]) # =&gt; [11, 12, 13] map(max, [1, 2, 3], [4, 2, 1]) # =&gt; [4, 2, 3] filter(lambda x: x &gt; 5, [3, 4, 5, 6, 7]) # =&gt; [6, 7] . We can use list comprehensions for nice maps and filters . [add_10(i) for i in [1, 2, 3]] # =&gt; [11, 12, 13] [x for x in [3, 4, 5, 6, 7] if x &gt; 5] # =&gt; [6, 7] . (Back to top) . . Classes . # We subclass from object to get a class. class Human(object): # A class attribute. It is shared by all instances of this class species = &quot;H. sapiens&quot; # Basic initializer, this is called when this class is instantiated. # Note that the double leading and trailing underscores denote objects # or attributes that are used by python but that live in user-controlled # namespaces. You should not invent such names on your own. def __init__(self, name): # Assign the argument to the instance&#39;s name attribute self.name = name # Initialize property self.age = 0 # An instance method. All methods take &quot;self&quot; as the first argument def say(self, msg): return &quot;{0}: {1}&quot;.format(self.name, msg) # A class method is shared among all instances # They are called with the calling class as the first argument @classmethod def get_species(cls): return cls.species # A static method is called without a class or instance reference @staticmethod def grunt(): return &quot;*grunt*&quot; # A property is just like a getter. # It turns the method age() into an read-only attribute # of the same name. @property def age(self): return self._age # This allows the property to be set @age.setter def age(self, age): self._age = age # This allows the property to be deleted @age.deleter def age(self): del self._age . # Instantiate a class i = Human(name=&quot;Ian&quot;) print i.say(&quot;hi&quot;) # prints out &quot;Ian: hi&quot; j = Human(&quot;Joel&quot;) print j.say(&quot;hello&quot;) # prints out &quot;Joel: hello&quot; # Call our class method i.get_species() # =&gt; &quot;H. sapiens&quot; # Change the shared attribute Human.species = &quot;H. neanderthalensis&quot; i.get_species() # =&gt; &quot;H. neanderthalensis&quot; j.get_species() # =&gt; &quot;H. neanderthalensis&quot; # Call the static method Human.grunt() # =&gt; &quot;*grunt*&quot; # Update the property i.age = 42 # Get the property i.age # =&gt; 42 # Delete the property del i.age i.age # =&gt; raises an AttributeError . (Back to top) . . Modules . # You can import modules import math print math.sqrt(16) # =&gt; 4 # You can get specific functions from a module from math import ceil, floor print ceil(3.7) # =&gt; 4.0 print floor(3.7) # =&gt; 3.0 # You can import all functions from a module. Warning: this is not recommended from math import * # You can shorten module names import math as m math.sqrt(16) == m.sqrt(16) # =&gt; True # you can also test that the functions are equivalent from math import sqrt math.sqrt == m.sqrt == sqrt # =&gt; True # own modules Python modules are just ordinary python files. You can write your own, and import them. The name of the module is the same as the name of the file. &lt;br&gt; You can find out which functions and attributes defines a module. import math dir(math) . (Back to top) . . Advanced . Generators . # Generators help you make lazy code def double_numbers(iterable): for i in iterable: yield i + i . (Back to top) . . A generator creates values on the fly. Instead of generating and returning all values at once it creates one in each iteration. This means values bigger than 15 wont be processed in double_numbers. Note xrange is a generator that does the same thing range does. Creating a list 1-900000000 would take lot of time and space to be made. xrange creates an xrange generator object instead of creating the entire list like range does. . We use a trailing underscore in variable names when we want to use a name that would normally collide with a python keyword . xrange_ = xrange(1, 900000000) # will double all numbers until a result &gt;=30 found for i in double_numbers(xrange_): print i if i &gt;= 30: break . (Back to top) . . Decorators . # in this example beg wraps say Beg will call say. If say_please is True then it will change the returned message from functools import wraps def beg(target_function): @wraps(target_function) def wrapper(*args, **kwargs): msg, say_please = target_function(*args, **kwargs) if say_please: return &quot;{} {}&quot;.format(msg, &quot;Please! I am poor :(&quot;) return msg return wrapper @beg def say(say_please=False): msg = &quot;Can you buy me a beer?&quot; return msg, say_please . (Back to top) . . Files . # iterate over files import glob,os.path filesDepth3 = glob.glob(&#39;*/*/*&#39;) #only dirs dirsDepth3 = filter(lambda f: os.path.isdir(f), filesDepth3) for file in list(glob.glob(&#39;*.txt&#39;)): reader = open(file) # join directory and filename os.path.join(dir_name, base_filename + &quot;.&quot; + filename_suffix) . Common operating system tasks source . ## Make a folder import os os.mkdir(&#39;mydir&#39;) ## Make intermediate folders foldername = os.path.join(os.environ[&#39;HOME&#39;], &#39;python&#39;, &#39;project1&#39;, &#39;temp&#39;) os.makedirs(foldername) ## Move to a folder origfolder = os.getcwd() # get name of current folder os.chdir(foldername) # move (&quot;change directory&quot;) ... os.chdir(origfolder) # move back ## Rename a file or folder os.rename(oldname, newname) ## List files import glob filelist1 = glob.glob(&#39;*.py&#39;) filelist2 = glob.glob(&#39;plot*[1-4]*.dat&#39;) ## List all files and folders in a folder filelist1 = os.listdir(&#39;mydir&#39;) filelist1 = os.listdir(os.curdir) # current folder (directory) filelist1.sort() # sort alphabetically ## Check if a file or folder exists if os.path.isfile(filename): inputfile = open(filename, &#39;r&#39;) ... if os.path.isdir(dirnamename): filelist = os.listdir(dirname) ... ## Remove files import glob filelist = glob.glob(&#39;tmp_*.pdf&#39;) for filename in filelist: os.remove(filename) ## Remove a folder and all its subfolders import shutil shutil.rmtree(foldername) ## Copy a file to another file or folder shutil.copy(&#39;fromfile&#39;, &#39;tofile&#39;) ## Copy a folder and all its subfolders shutil.copytree(sourcefolder, destination) . Command line . print(subprocess.check_output([converter_script, &#39;-f&#39; ,input_file,&#39;-o&#39;, output_file])) . Printing . use f strings: . &gt;&gt;&gt; f&quot;{name.lower()} is funny.&quot; &#39;eric idle is funny.&#39; . source . Images . Display images . import Image image = Image.open(&#39;File.jpg&#39;) image.show() img = PIL.Image.open(fn); img . convert array to image . from PIL import Image import numpy as np w, h = 512, 512 data = np.zeros((h, w, 3), dtype=np.uint8) data[256, 256] = [255, 0, 0] img = Image.fromarray(data, &#39;RGB&#39;) img.save(&#39;my.png&#39;) img.show() .",
            "url": "/blog/python/cheat%20sheet/2020/03/03/python-cheat-sheet.html",
            "relUrl": "/python/cheat%20sheet/2020/03/03/python-cheat-sheet.html",
            "date": " • Mar 3, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # Title &gt; Awesome summary - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
  
    
        ,"post10": {
            "title": "Big Data Concepts Storage",
            "content": ". | Storage HDFS Architecture | Datanode | Journalnodes | Read Path | Write Path | HDFS federation Active/passive Namenode | High availability Shared Storage using NFS | Quorum-based Storage | . | . | . | NoSQL Concepts Row- vs. column-oriented | . | HBase Overview | Advantages | ACID | Schema design Sparse | . | Write Path | Read Path | Architecture | Table | Regions | Region server | Region splitting | Region splitting process | Perfomance tuning | Snapshots | . | Phoenix where it fits in | Architecture | . | Cassandra Data nodes | Read Path Within node | . | Write Path | Partitioning and Replication | Hashing / Tokens | Security | . | Graph databases Titan Architecture | . | Tinkerpop Architecture | Graph landscape | Getting started - Example graph | . | . | Comparisons Cassandra vs HBase vs MongoDB vs Couchbase Update | Cap Continuum | . | . | . | . | SQL on hadoop BigSQL Architecture | . | Hive HiveQL | DDL | HCatalog | Web Interface Hive Web Interface | WebHCat | . | . | Tez Tez vs Spark | Hive with LLAP (Live Long and Process) | MapReduce vs Tez vs Tez LLAP | . | Drill | Impala | Comparison Impala, Hive, Stinger, Pivotal, Teradata, Presto | Benchmark: Impala, Hive, Spark | Benchmark: Impala, Hive-on-Tez | . | Overview | . | . Storage . HDFS . Architecture . image source . (Back to top) . . Datanode . image source . (Back to top) . . Journalnodes . In order for the Standby node to keep its state synchronized with the Active node, both nodes communicate with a group of separate daemons called “JournalNodes”. . Active node logs namespace modification to majority of JNs. | Standby node can read edits from JNs and is constantly watching for changes in edit log | . Read Path . . image source . The client using a Distributed FileSystem object of Hadoop client API calls open() which initiate the read request. | Distributed FileSystem connects with NameNode. NameNode identifies the block locations of the file to be read and in which DataNodes the block is located. | NameNode then sends the list of DataNodes in order of nearest DataNodes from the client. | Distributed FileSystem then creates FSDataInputStream objects, which, in turn, wrap a DFSInputStream, which can connect to the DataNodes selected and get the block, and return to the client. The client initiates the transfer by calling the read() of FSDataInputStream. | FSDataInputStream repeatedly calls the read() method to get the block data. | When the end of the block is reached, DFSInputStream closes the connection from the DataNode and identifies the best DataNode for the next block. | When the client has finished reading, it will call close() on FSDataInputStream to close the connection. | (Back to top) . . Write Path . . image source . The client, using a Distributed FileSystem object of Hadoop client API, calls create(), which initiates the write request. | Distributed FileSystem connects with NameNode. NameNode initiates a new file creation, and creates a new record in metadata and initiates an output stream of type FSDataOutputStream, which wraps DFSOutputStream and returns it to the client. Before initiating the file creation, NameNode checks if a file already exists and whether the client has permissions to create a new file and if any of the condition is true then an IOException is thrown to the client. | The client uses the FSDataOutputStream object to write the data and calls the write() method. The FSDataOutputStream object, which is DFSOutputStream, handles the communication with the DataNodes and NameNode. | DFSOutputStream splits files to blocks and coordinates with NameNode to identify the DataNode and the replica DataNodes. The number of the replication factor will be the number of DataNodes identified. Data will be sent to a DataNode in packets, and that DataNode will send the same packet to the second DataNode, the second DataNode will send it to the third, and so on, until the number of DataNodes is identified. | When all the packets are received and written, DataNodes send an acknowledgement packet to the sender DataNode, to the client. DFSOutputStream maintains a queue internally to check if the packets are successfully written by DataNode. DFSOutputStream also handles if the acknowledgment is not received or DataNode fails while writing. | If all the packets have been successfully written, then the client closes the stream. | If the process is completed, then the Distributed FileSystem object notifies the NameNode of the status. | (Back to top) . . HDFS federation . . image source . (Back to top) . . Active/passive Namenode . Active Namenode is the one who is writing edits to JournalNodes. | DataNodes know about location of both Namenodes, they send block location information and heartbeats to both. | Passive Namenode performs checkpoints (of namespace state), no Secondary NameNode required. | . (Back to top) . . High availability . In general, there are two approaches . Shared Storage using NFS | Quorum-based Storage | . Shared Storage using NFS . . image source . (Back to top) . . Quorum-based Storage . . image source . adds two new components to HDFS deployment . Zookeeper quorum | ZKFailoverController (ZKFC) | . Automatic failover relies on Zookeeper: . Each Namenode maintains persistent session in Zookeeper | If machine crashes, session expires | Other Namenode is notified to take over | . ZKFC is responsible for: . ZKFC pings its local Namenode. If unresponsive, ZKFC marks it as unhealthy. | . (Back to top) . . NoSQL . Concepts . Row- vs. column-oriented . . image source . (Back to top) . . HBase . Overview . Sparse: HBase is columnar and partition oriented. Usually, a record may have many columns and many of them may have null data, or the values may be repeated. HBase can efficiently and effectively save the space in sparse data. | Distributed: Data is stored in multiple nodes, scattered across the cluster. | Persistent: Data is written and saved in the cluster. | Multidimensional: A row can have multiple versions or timestamps of values. | Map: Key-Value Pair links the data structure to store the data. | Sorted: The Key in the structure is stored in a sorted order for faster read and write optimization. | . Advantages . Need of real-time random read/write on a high scale | Variable Schema: columns can be added or removed at runtime (schema-on-read) | Many columns of the datasets are sparse | Key based retrieval and auto sharding is required | Need of consistency more than availability | Data or tables have to be denormalized for better performance | . ACID . Atomicity: An operation in HBase either completes entirely or not at all for a row, but across nodes it is eventually consistent. | Durability: An update in HBase will not be lost due to WAL and MemStore. | Consistency and Isolation HBase is strongly consistent for a single row level but not across levels. | . (further reading) . (Back to top) . . Schema design . Sparse . “Sparse” means that for any given row you can have one or more columns, but each row doesn’t need to have all the same columns as other rows like it (as in a relational model) . . image source . (Back to top) . . Write Path . . image source . Client requests data to be written in HTable, the request comes to a RegionServer. | The RegionServer writes the data first in WAL. | The RegionServer identifies the Region which will store the data and the data will be saved in MemStore of that Region. | MemStore holds the data in memory and does not persist it. When the threshold value reaches in the MemStore, then the data is flushed as a HFile in that region. | (Back to top) . . Read Path . . image source . Client sends a read request. The request is received by the RegionServer which identifies all the Regions where the HFiles are present. | First, the MemStore of the Region is queried; if the data is present, then the request is serviced. | If the data is not present, the BlockCache is queried to check if it has the data; if yes, the request is serviced. | If the data is not present in the BlockCache, then it is pulled from the Region and serviced. Now the data is cached in MemStore and BlockCache.. | Architecture . . image source . . image source . . image source . (Back to top) . . Table . An HBase table comprises a set of metadata information and a set of key/value pairs: . Table Info: A manifest file that describes the table “settings”, like column families, compression and encoding codecs, bloom filter types, and so on. | Regions: The table “partitions” are called regions. Each region is responsible for handling a contiguous set of key/values, and they are defined by a start key and end key. | WALs/MemStore: Before writing data on disk, puts are written to the Write Ahead Log (WAL) and then stored in-memory until memory pressure triggers a flush to disk. The WAL provides an easy way to recover puts not flushed to disk on failure. | HFiles: At some point all the data is flushed to disk; an HFile is the HBase format that contains the stored key/values. HFiles are immutable but can be deleted on compaction or region deletion. | . source . . image source . (Back to top) . . Regions . . image source . (Back to top) . . Region server . . image source . . image source . (Back to top) . . Region splitting . . image source . (Back to top) . . Region splitting process . . image source . RegionServer decides locally to split the region, and prepares the split. As a first step, it creates a znode in zookeeper under /hbase/region-in-transition/region-name in SPLITTING state. | The Master learns about this znode, since it has a watcher for the parent region-in-transition znode. | RegionServer creates a sub-directory named “.splits” under the parent’s region directory in HDFS. | RegionServer closes the parent region, forces a flush of the cache and marks the region as offline in its local data structures. At this point, client requests coming to the parent region will throw NotServingRegionException. The client will retry with some backoff. | RegionServer create the region directories under .splits directory, for daughter regions A and B, and creates necessary data structures. Then it splits the store files, in the sense that it creates two Reference files per store file in the parent region. Those reference files will point to the parent regions files. | RegionServer creates the actual region directory in HDFS, and moves the reference files for each daughter. | RegionServer sends a Put request to the .META. table, and sets the parent as offline in the .META. table and adds information about daughter regions. At this point, there won’t be individual entries in .META. for the daughters. Clients will see the parent region is split if they scan .META., but won’t know about the daughters until they appear in .META.. Also, if this Put to .META. succeeds, the parent will be effectively split. If the RegionServer fails before this RPC succeeds, Master and the next region server opening the region will clean dirty state about the region split. After the .META. update, though, the region split will be rolled-forward by Master. | RegionServer opens daughters in parallel to accept writes. | RegionServer adds the daughters A and B to .META. together with information that it hosts the regions. After this point, clients can discover the new regions, and issue requests to the new region. Clients cache the .META. entries locally, but when they make requests to the region server or .META., their caches will be invalidated, and they will learn about the new regions from .META.. | RegionServer updates znode /hbase/region-in-transition/region-name in zookeeper to state SPLIT, so that the master can learn about it. The balancer can freely re-assign the daughter regions to other region servers if it chooses so. | After the split, meta and HDFS will still contain references to the parent region. Those references will be removed when compactions in daughter regions rewrite the data files. Garbage collection tasks in the master periodically checks whether the daughter regions still refer to parents files. If not, the parent region will be removed. | source . (Back to top) . . Perfomance tuning . Compression | Filters | Counters | HBase co-processors | . (Back to top) . . Snapshots . A snapshot is a set of metadata information that allows an admin to get back to a previous state of the table. A snapshot is not a copy of the table; it’s just a list of file names and doesn’t copy the data. A full snapshot restore means that you get back to the previous “table schema” and you get back your previous data losing any changes made since the snapshot was taken . Recovery from user/application errors | Restore/Recover from a known safe state. | View previous snapshots and selectively merge the difference into production. | Save a snapshot right before a major application upgrade or change. | Auditing and/or reporting on views of data at specific time | Capture monthly data for compliance purposes. | Run end-of-day/month/quarter reports. | Application testing | Test schema or application changes on data similar to that in production from a snapshot and then throw it away. For example: take a snapshot, create a new table from the snapshot content (schema plus data), and manipulate the new table by changing the schema, adding and removing rows, and so on. (The original table, the snapshot, and the new table remain mutually independent.) | Offloading of work | Take a snapshot, export it to another cluster, and run your MapReduce jobs. Since the export snapshot operates at HDFS level, you don’t slow down your main HBase cluster as much as CopyTable does. | . source . . image source . Phoenix . Apache Phoenix enables OLTP and operational analytics in Hadoop for low latency applications by combining the best of both worlds: . The power of standard SQL and JDBC APIs with full ACID transaction capabilities . | The flexibility of late-bound, schema-on-read capabilities from the NoSQL world by leveraging HBase as its backing store . | . Apache Phoenix is fully integrated with other Hadoop products such as Spark, Hive, Pig, Flume, and Map Reduce. . where it fits in . . image source . Architecture . . image source . (Back to top) . . Cassandra . Data nodes . . image source . (Back to top)&lt;hr&gt; . Read Path . . image source . Within node . . image source . (Back to top)&lt;hr&gt; . Write Path . . image source . . image source . (Back to top)&lt;hr&gt; . Partitioning and Replication . . image source . (Back to top)&lt;hr&gt; . Hashing / Tokens . . image source . (Back to top)&lt;hr&gt; . Security . image source . (Back to top)&lt;hr&gt; . Graph databases . Titan . Architecture . . image source . &lt;dependency&gt; &lt;groupId&gt;com.thinkaurelius.titan&lt;/groupId&gt; &lt;artifactId&gt;titan-core&lt;/artifactId&gt; &lt;version&gt;1.0.0&lt;/version&gt; &lt;/dependency&gt; &lt;!-- core, all, cassandra, hbase, berkeleyje, es, lucene --&gt; . // who is hercules&#39; grandfather? g.V.has(&#39;name&#39;,&#39;hercules&#39;).out(&#39;father&#39;).out(&#39;father&#39;).name . Tinkerpop . The goal of TinkerPop, as a Graph Computing Framework, is to make it easy for developers to create graph applications by providing APIs and tools that simplify their endeavors. . Architecture . image source . Graph landscape . image source . (Back to top) . . Getting started - Example graph . image source . Gremlin get started . (Back to top) . . Comparisons . Cassandra vs HBase vs MongoDB vs Couchbase . . image source . . image source . (Back to top) . . Update . . image source . (Back to top) . . Cap Continuum . . image source . In this depiction, relational databases are on the line between Consistency and Availability, which means that they can fail in the event of a network failure (including a cable breaking). This is typically achieved by defining a single master server, which could itself go down, or an array of servers that simply don’t have sufficient mechanisms built in to continue functioning in the case of network partitions. . Graph databases such as Neo4J and the set of databases derived at least in part from the design of Google’s Bigtable database (such as MongoDB, HBase, Hypertable, and Redis) all are focused slightly less on Availability and more on ensuring Consistency and Partition Tolerance. . Finally, the databases derived from Amazon’s Dynamo design include Cassandra, Project Voldemort, CouchDB, and Riak. These are more focused on Availability and Partition-Tolerance. . (Back to top) . . SQL on hadoop . BigSQL . Architecture . image source . image source . Hive . image source . image source . (Back to top) . . HiveQL . DDL . HCatalog . Built on top of Hive Metastore | Incorporates components from Hive DDL | . Web Interface . Hive Web Interface . Functions: . Schema browsing | Detached Query execution | No local installation | . WebHCat . Functions: . applications can make HTTP requests to access the Hive metastore | create or queue Hive queries and commands | Pig jobs | MapReduce or Yarn jobs | . Tez . Apache Tez is part of the Stinger initiative. Tez generalizes the MapReduce paradigm to a more powerful framework based on expressing computations as a dataflow graph. Tez exists to address some of the limitations of MapReduce. For example, in a typical MapReduce, a lot of temporary data is stored (such as each mapper’s output, which is a disk I/O), which is an overhead. In the case of Tez, this disk I/O of temporary data is saved, thereby resulting in higher performance compared to the MapReduce model. . Tez is a framework for purpose-built tools such as Hive and Pig. . image source . image source . image source . Tez vs Spark . Tez vs Spark Flink on Tez . Hive with LLAP (Live Long and Process) . With 2.1 Hive introduces LLAP, a daemon layer for sub-second queries. LLAP combines persistent query servers and optimized in-memory caching that allows Hive to launch queries instantly and avoids unnecessary disk I/O. For more information see http://hortonworks.com/blog/announcing-apache-hive-2-1-25x-faster-queries-much/. . image source . MapReduce vs Tez vs Tez LLAP . Tez LLAP process compared to Tez execution process and MapReduce process image source . Drill . not tied into Hadoop | more flexible query &amp; execution layer | still in incubation | . Impala . dependent on Hadoop | dependent on Hive Metastore | rumour to not fall back to disk when memory exceeded | . Comparison . Impala, Hive, Stinger, Pivotal, Teradata, Presto . image source . (Back to top) . . Benchmark: Impala, Hive, Spark . source . (Back to top) . . Benchmark: Impala, Hive-on-Tez . source . (Back to top) . . Overview . . image source . (Back to top) . .",
            "url": "/blog/bigdata/2016/09/14/big-data-storage.html",
            "relUrl": "/bigdata/2016/09/14/big-data-storage.html",
            "date": " • Sep 14, 2016"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "👤️ About Me",
          "content": "I am a data scientist and bioinformatician by training. My research interest includes . bioinformatics | AI for drug discovery | precision medicine | computer vision applications of Deep Learning | model interpretability. | . Education . PhD in Bioinformatics, 2010, GGNB Goettingen | MSc in Bioinformatics, 2007, Georg August University Goettingen | BSc in Bioinformatics, 2005, Georg August University Goettingen | . Links . Twitter Linkedin Google scholar . This blog is powered by fastpages .",
          "url": "/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  
      ,"page6": {
          "title": "Code snippets",
          "content": "I keep a collection of useful code snippets in . Python data science cheat sheet: Input/Output, Merge/join, Merge/join, Dates, Missing data, Manipulations, Method chaining, Iterate rows, Concat vs append, Parallel data structures, Jupyter notebooks, Jupyter code snippets, Testing | Python regular cheat sheet: Primitive datatypes and operators, Strings, Lists, Tuples, Dictionary, Sets, Control flow, Exceptions, Functions, Classes, Modules, Generators, Decorators, Files | R cheat sheet: gist link | Other: Unix, Docker, Git | . AI Course notes: Fast.ai DL course (notebooks, notes), Fast.ai ML course (notebooks, notes),&lt;/p&gt; Andrew NG DL course (drawings), Andrew NG ML course, Sebastian Raschka, Data science book . Jupyter use cases . &lt;/li&gt; Deep learning: History, Conv layer, Convs explained, Autoencoder, Reinforcement learning | . NLP: NER, word embeddings, | . Reinforcement learning: Reinforcement learning, | . Collection of DS articles: link | . Kaggle: Competitions notes, Participated competitions | &lt;/ul&gt; . Statistics Notes | . Biomedical Science AI in drug discovery: link | AI4Medicine: link | Medicinal chemistry: pre-regulatory medicine, protein structure, pharmacokinetics, enzymes, receptors, metabolism, lead optimization, admet, admet assays explained, lipophilicity, pka, solubility, permeability, metabolic stability | &lt;/p&gt; Biology: miRNA, Sequencing | . Clinical trials: Clinical trial notes | . Organic chemistry course (khan academy): Structure and bonding, Electronegativity, Alkanes cycloalkanes and functional-groups, Functional groups, Stereochemistry, | Organic chemistry: link | &lt;/ul&gt; . Other Krav maga | Facts: Climate change, Vaccines, Nutrition, GMO | Books: Brain rules for baby, No drama discipline, Thinking fast and slow, Factfullness | . Big data Best practices: HBase, Kafka | Hadoop security | . . | .",
          "url": "/blog/snippets/",
          "relUrl": "/snippets/",
          "date": ""
      }
      
  

  

  
  

  
  

}